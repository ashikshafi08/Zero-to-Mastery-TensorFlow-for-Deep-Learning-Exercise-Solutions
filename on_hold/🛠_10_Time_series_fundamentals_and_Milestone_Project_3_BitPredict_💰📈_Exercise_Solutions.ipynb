{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ðŸ› _10. Time series fundamentals and Milestone Project 3: BitPredict ðŸ’°ðŸ“ˆ Exercise Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKXjnNYFm0bffuUaT/GyGd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Zero-to-Mastery-TensorFlow-for-Deep-Learning-Exercise-Solutions/blob/main/on_hold/%F0%9F%9B%A0_10_Time_series_fundamentals_and_Milestone_Project_3_BitPredict_%F0%9F%92%B0%F0%9F%93%88_Exercise_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9BhPZ1Tx4bm"
      },
      "source": [
        "# ðŸ›  10. Time series fundamentals and Milestone Project 3: BitPredict ðŸ’°ðŸ“ˆ Exercise Solutions \n",
        "\n",
        "1. Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 & 1)\n",
        "    * Try doing this for a univariate model (e.g. model_1) and a multivariate model (e.g. model_6) and see if it effects model training or evaluation results.\n",
        "2. Get the most up to date data on Bitcoin, train a model & see how it goes (our data goes up to May 18 2021).\n",
        "    * You can download the Bitcoin historical data for free from  [coindesk.com/price/bitcoin](https://www.coindesk.com/price/bitcoin)  and clicking â€œExport Dataâ€ -> â€œCSVâ€.\n",
        "3. For most of our models we used WINDOW_SIZE=7, but is there a better window size?\n",
        "    * Setup a series of experiments to find whether or not thereâ€™s a better window size.\n",
        "    * For example, you might train 10 different models with HORIZON=1 but with window sizes ranging from 2-12.\n",
        "4. Create a windowed dataset just like the ones we used for model_1 using  [tf.keras.preprocessing.timeseries_dataset_from_array()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)  and retrain model_1 using the recreated dataset.\n",
        "5. For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.\n",
        "    * Are there any other features you think you could add?\n",
        "    * If so, try it out, how do these affect the model?\n",
        "6. Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for model_8.\n",
        "7. For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasnâ€™t retrained for every forecast (model_9)?\n",
        "8. Throughout this notebook, weâ€™ve only tried algorithms weâ€™ve handcrafted ourselves. But itâ€™s worth seeing how a purpose built forecasting algorithm goes.\n",
        "    * Try out one of the extra algorithms listed in the modelling experiments part such as:\n",
        "\t*  [Facebookâ€™s Kats library](https://github.com/facebookresearch/Kats)  - there are many models in here, remember the machine learning practionerâ€™s motto: experiment, experiment, experiment.\n",
        "\t*  [LinkedInâ€™s Greykite library](https://github.com/linkedin/greykite) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT9614RIyCCl"
      },
      "source": [
        "## Downloading the data and preprocessing it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "aZe0Y63IyQgT",
        "outputId": "5bb35957-c85f-40fd-9484-1a1a5ece2b66"
      },
      "source": [
        "# Download Bitcoin historical data from GitHub \n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\n",
        "\n",
        "# Import with pandas \n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \n",
        "                 parse_dates=[\"Date\"], \n",
        "                 index_col=[\"Date\"]) # parse the date column (tell pandas column 1 is a datetime)\n",
        "df.head()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-12 15:20:32--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178509 (174K) [text/plain]\n",
            "Saving to: â€˜BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv.1â€™\n",
            "\n",
            "\r          BTC_USD_2   0%[                    ]       0  --.-KB/s               \rBTC_USD_2013-10-01_ 100%[===================>] 174.33K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-09-12 15:20:32 (9.09 MB/s) - â€˜BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv.1â€™ saved [178509/178509]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Currency</th>\n",
              "      <th>Closing Price (USD)</th>\n",
              "      <th>24h Open (USD)</th>\n",
              "      <th>24h High (USD)</th>\n",
              "      <th>24h Low (USD)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>BTC</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>124.30466</td>\n",
              "      <td>124.75166</td>\n",
              "      <td>122.56349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>BTC</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>125.75850</td>\n",
              "      <td>123.63383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>BTC</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>125.66566</td>\n",
              "      <td>83.32833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>BTC</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>118.67500</td>\n",
              "      <td>107.05816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>BTC</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>121.93633</td>\n",
              "      <td>118.00566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Currency  Closing Price (USD)  ...  24h High (USD)  24h Low (USD)\n",
              "Date                                      ...                               \n",
              "2013-10-01      BTC            123.65499  ...       124.75166      122.56349\n",
              "2013-10-02      BTC            125.45500  ...       125.75850      123.63383\n",
              "2013-10-03      BTC            108.58483  ...       125.66566       83.32833\n",
              "2013-10-04      BTC            118.67466  ...       118.67500      107.05816\n",
              "2013-10-05      BTC            121.33866  ...       121.93633      118.00566\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LQW1xGSyYDv",
        "outputId": "565e1a8e-dea8-4c52-e12c-6bc672ff420b"
      },
      "source": [
        "# Only want closing price for each day \n",
        "bitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\n",
        "bitcoin_prices.head() , bitcoin_prices.shape"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                Price\n",
              " Date                 \n",
              " 2013-10-01  123.65499\n",
              " 2013-10-02  125.45500\n",
              " 2013-10-03  108.58483\n",
              " 2013-10-04  118.67466\n",
              " 2013-10-05  121.33866, (2787, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIlIWPTWD6z_"
      },
      "source": [
        "# Get the data in array \n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import layers\n",
        "\n",
        "timesteps = bitcoin_prices.index.to_numpy()\n",
        "prices = bitcoin_prices['Price'].to_numpy()\n",
        "\n",
        "# Instantiating the sklearn MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler() "
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eynNXXID68q"
      },
      "source": [
        "# Create function to view NumPy arrays as windows \n",
        "\n",
        "def get_labelled_windows(x , horizon):\n",
        "  return x[:, :-horizon] ,x[: , -horizon:]\n",
        "\n",
        "\n",
        "def make_windows_scaled(x, window_size=7, horizon=1):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential windows of window_size. Also applies the standard scaler\n",
        "  \"\"\"\n",
        "  scaler.fit(np.expand_dims(x , axis =1))\n",
        "  scaled_x = scaler.transform(np.expand_dims(x , axis = 1))\n",
        "  scaled_x = np.squeeze(scaled_x)\n",
        "  \n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(scaled_x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
        "  windowed_array = scaled_x[window_indexes]\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "  return windows, labels\n",
        "\n",
        "\n",
        "# Make the splits \n",
        "def make_train_test_splits(windows , labels , test_split = 0.2):\n",
        "  split_size = int(len(windows) * (1 - test_split))\n",
        "  train_windows = windows[:split_size]\n",
        "  train_labels = labels[:split_size]\n",
        "  test_windows = windows[split_size:]\n",
        "  test_labels = labels[split_size:]\n",
        "\n",
        "  return train_windows ,  test_windows ,train_labels,  test_labels"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dItohLM1Ozr"
      },
      "source": [
        "### 1. Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 & 1)\n",
        "* Try doing this for a univariate model (e.g. model_1) and a multivariate model (e.g. model_6) and see if it effects model training or evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S-J_3Tkzfto",
        "outputId": "88ef995c-74ca-4a88-823f-4b58987807fc"
      },
      "source": [
        "# Model 1 (Horizon = 1 , Window_size = 7)\n",
        "HORIZON = 1 \n",
        "WINDOW_SIZE = 7 \n",
        "\n",
        "\n",
        "full_windows , full_labels = make_windows_scaled(prices , window_size = WINDOW_SIZE , horizon = HORIZON)\n",
        "full_windows.shape , full_labels.shape"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2780, 7), (2780, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL8e6Iz9NOUX",
        "outputId": "d36152c1-f15d-4af8-afab-ed2ffbb1ecf7"
      },
      "source": [
        "# Looking at few examples of how price is scaled\n",
        "for i in range(3):\n",
        "  print(f'Window: {full_windows[i]} --> Label {full_labels[i]}')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window: [0.00023831 0.00026677 0.         0.00015955 0.00020168 0.00019087\n",
            " 0.0002089 ] --> Label [0.00022847]\n",
            "Window: [0.00026677 0.         0.00015955 0.00020168 0.00019087 0.0002089\n",
            " 0.00022847] --> Label [0.00024454]\n",
            "Window: [0.         0.00015955 0.00020168 0.00019087 0.0002089  0.00022847\n",
            " 0.00024454] --> Label [0.00027478]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmHd26QZFZDU",
        "outputId": "e9c6aa09-13f1-4cf7-853f-58b83d5a5fe0"
      },
      "source": [
        "# Making train and test splits \n",
        "train_windows , test_windows , train_labels , test_labels = make_train_test_splits(full_windows , full_labels)\n",
        "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2224, 556, 2224, 556)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2dT3_hqyejt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63e599b-10c2-4675-9823-cadf72b14f99"
      },
      "source": [
        "# Building the Model 1 \n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Construct the model \n",
        "model_1 = tf.keras.Sequential([\n",
        "  layers.Dense(128, activation= 'relu') ,\n",
        "  layers.Dense(HORIZON , activation = 'linear')\n",
        "])\n",
        "\n",
        "# Compiling the model \n",
        "model_1.compile(loss = 'mae' , \n",
        "                optimizer = tf.keras.optimizers.Adam() , \n",
        "                metrics = ['mae'])\n",
        "\n",
        "# Fit the model \n",
        "model_1.fit(x = train_windows , \n",
        "            y = train_labels , \n",
        "            epochs = 100 , batch_size = 128 , verbose = 0 , \n",
        "            validation_data = (test_windows , test_labels))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5111ae13d0>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MwuA2gI0t2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89319dad-15b1-48f3-dbca-81d4072daafe"
      },
      "source": [
        "# Evaluate the model on test data \n",
        "model_1.evaluate(test_windows , test_labels)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 1ms/step - loss: 0.0102 - mae: 0.0102\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01022376399487257, 0.01022376399487257]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh7sqPOjNo8V"
      },
      "source": [
        "# Making predictions \n",
        "model_1_preds = tf.squeeze(model_1.predict(test_windows))"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMNXeHUWXcEI"
      },
      "source": [
        "Now doing the same for the Multivariate data especially for the Model 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "6HoW1S6TXhvD",
        "outputId": "eb6d2735-ed5e-45ab-a775-2dd9c90a7fe3"
      },
      "source": [
        "# Block reward values\n",
        "block_reward_1 = 50 # 3 January 2009 \n",
        "block_reward_2 = 25 # 28 November 2012 \n",
        "block_reward_3 = 12.5 # 9 July 2016\n",
        "block_reward_4 = 6.25 # 11 May 2020\n",
        "\n",
        "# Block reward dates (datetime form of the above date stamps)\n",
        "block_reward_2_datetime = np.datetime64(\"2012-11-28\")\n",
        "block_reward_3_datetime = np.datetime64(\"2016-07-09\")\n",
        "block_reward_4_datetime = np.datetime64(\"2020-05-11\")\n",
        "\n",
        "# Get date indexes for when to add in different block dates\n",
        "block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\n",
        "block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\n",
        "block_reward_2_days, block_reward_3_days\n",
        "\n",
        "# Add block_reward column\n",
        "bitcoin_prices_block = bitcoin_prices.copy()\n",
        "bitcoin_prices_block[\"block_reward\"] = None\n",
        "\n",
        "# Set values of block_reward column (it's the last column hence -1 indexing on iloc)\n",
        "bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\n",
        "bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\n",
        "bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\n",
        "bitcoin_prices_block.head()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>block_reward</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>123.65499</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>125.45500</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>108.58483</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>118.67466</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>121.33866</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Price block_reward\n",
              "Date                              \n",
              "2013-10-01  123.65499           25\n",
              "2013-10-02  125.45500           25\n",
              "2013-10-03  108.58483           25\n",
              "2013-10-04  118.67466           25\n",
              "2013-10-05  121.33866           25"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "4NQkP9EmX2Xt",
        "outputId": "a1145e66-019d-4679-c1ae-2f986e7e8fda"
      },
      "source": [
        "# Make a copy of the Bitcoin historical data with block reward feature\n",
        "bitcoin_prices_windowed = bitcoin_prices_block.copy()\n",
        "\n",
        "# Add windowed columns\n",
        "for i in range(WINDOW_SIZE): \n",
        "  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
        "bitcoin_prices_windowed.head(10)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>block_reward</th>\n",
              "      <th>Price+1</th>\n",
              "      <th>Price+2</th>\n",
              "      <th>Price+3</th>\n",
              "      <th>Price+4</th>\n",
              "      <th>Price+5</th>\n",
              "      <th>Price+6</th>\n",
              "      <th>Price+7</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>123.65499</td>\n",
              "      <td>25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>125.45500</td>\n",
              "      <td>25</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>108.58483</td>\n",
              "      <td>25</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>118.67466</td>\n",
              "      <td>25</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>121.33866</td>\n",
              "      <td>25</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-06</th>\n",
              "      <td>120.65533</td>\n",
              "      <td>25</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-07</th>\n",
              "      <td>121.79500</td>\n",
              "      <td>25</td>\n",
              "      <td>120.65533</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-08</th>\n",
              "      <td>123.03300</td>\n",
              "      <td>25</td>\n",
              "      <td>121.79500</td>\n",
              "      <td>120.65533</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-09</th>\n",
              "      <td>124.04900</td>\n",
              "      <td>25</td>\n",
              "      <td>123.03300</td>\n",
              "      <td>121.79500</td>\n",
              "      <td>120.65533</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-10</th>\n",
              "      <td>125.96116</td>\n",
              "      <td>25</td>\n",
              "      <td>124.04900</td>\n",
              "      <td>123.03300</td>\n",
              "      <td>121.79500</td>\n",
              "      <td>120.65533</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Price block_reward    Price+1  ...    Price+5    Price+6    Price+7\n",
              "Date                                           ...                                 \n",
              "2013-10-01  123.65499           25        NaN  ...        NaN        NaN        NaN\n",
              "2013-10-02  125.45500           25  123.65499  ...        NaN        NaN        NaN\n",
              "2013-10-03  108.58483           25  125.45500  ...        NaN        NaN        NaN\n",
              "2013-10-04  118.67466           25  108.58483  ...        NaN        NaN        NaN\n",
              "2013-10-05  121.33866           25  118.67466  ...        NaN        NaN        NaN\n",
              "2013-10-06  120.65533           25  121.33866  ...  123.65499        NaN        NaN\n",
              "2013-10-07  121.79500           25  120.65533  ...  125.45500  123.65499        NaN\n",
              "2013-10-08  123.03300           25  121.79500  ...  108.58483  125.45500  123.65499\n",
              "2013-10-09  124.04900           25  123.03300  ...  118.67466  108.58483  125.45500\n",
              "2013-10-10  125.96116           25  124.04900  ...  121.33866  118.67466  108.58483\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "73jAFAVeYpD-",
        "outputId": "63dab29f-0e22-459f-f4c3-e0959db13e8f"
      },
      "source": [
        "# Let's create X & y, remove the NaN's and convert to float32 to prevent TensorFlow errors \n",
        "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32) \n",
        "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
        "X.head()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block_reward</th>\n",
              "      <th>Price+1</th>\n",
              "      <th>Price+2</th>\n",
              "      <th>Price+3</th>\n",
              "      <th>Price+4</th>\n",
              "      <th>Price+5</th>\n",
              "      <th>Price+6</th>\n",
              "      <th>Price+7</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-10-08</th>\n",
              "      <td>25.0</td>\n",
              "      <td>121.794998</td>\n",
              "      <td>120.655327</td>\n",
              "      <td>121.338661</td>\n",
              "      <td>118.674660</td>\n",
              "      <td>108.584831</td>\n",
              "      <td>125.455002</td>\n",
              "      <td>123.654991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-09</th>\n",
              "      <td>25.0</td>\n",
              "      <td>123.032997</td>\n",
              "      <td>121.794998</td>\n",
              "      <td>120.655327</td>\n",
              "      <td>121.338661</td>\n",
              "      <td>118.674660</td>\n",
              "      <td>108.584831</td>\n",
              "      <td>125.455002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-10</th>\n",
              "      <td>25.0</td>\n",
              "      <td>124.049004</td>\n",
              "      <td>123.032997</td>\n",
              "      <td>121.794998</td>\n",
              "      <td>120.655327</td>\n",
              "      <td>121.338661</td>\n",
              "      <td>118.674660</td>\n",
              "      <td>108.584831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-11</th>\n",
              "      <td>25.0</td>\n",
              "      <td>125.961159</td>\n",
              "      <td>124.049004</td>\n",
              "      <td>123.032997</td>\n",
              "      <td>121.794998</td>\n",
              "      <td>120.655327</td>\n",
              "      <td>121.338661</td>\n",
              "      <td>118.674660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-12</th>\n",
              "      <td>25.0</td>\n",
              "      <td>125.279663</td>\n",
              "      <td>125.961159</td>\n",
              "      <td>124.049004</td>\n",
              "      <td>123.032997</td>\n",
              "      <td>121.794998</td>\n",
              "      <td>120.655327</td>\n",
              "      <td>121.338661</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            block_reward     Price+1  ...     Price+6     Price+7\n",
              "Date                                  ...                        \n",
              "2013-10-08          25.0  121.794998  ...  125.455002  123.654991\n",
              "2013-10-09          25.0  123.032997  ...  108.584831  125.455002\n",
              "2013-10-10          25.0  124.049004  ...  118.674660  108.584831\n",
              "2013-10-11          25.0  125.961159  ...  121.338661  118.674660\n",
              "2013-10-12          25.0  125.279663  ...  120.655327  121.338661\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SdPXx_SauLC"
      },
      "source": [
        "# Scaling the X data \n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(np.expand_dims(y , axis = 1))\n",
        "y_scaled = np.squeeze(y_scaled)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M8UVLaIa1km",
        "outputId": "92fe1cad-2772-4a3b-b92b-82e83f8bbf89"
      },
      "source": [
        "# Make train and test set splits of the scaled data \n",
        "split_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X_scaled[:split_size], y_scaled[:split_size]\n",
        "X_test, y_test = X_scaled[split_size:], y_scaled[split_size:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2224, 2224, 556, 556)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sXgRZ1Ka-TT",
        "outputId": "32a6ed6d-814d-4554-d8f5-3e01a6927114"
      },
      "source": [
        "# Building a Multivariate time series model and fitting it\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_6 = tf.keras.Sequential([\n",
        "  layers.Dense(128 , activation= 'relu'), \n",
        "  layers.Dense(HORIZON)\n",
        "])\n",
        "\n",
        "model_6.compile(loss = 'mae' , \n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "model_6.fit(X_train , y_train , \n",
        "          epochs = 100 ,\n",
        "          verbose = 0 , batch_size = 128, \n",
        "          validation_data = (X_test , y_test))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5111cff450>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs_ZDtr1b3-N",
        "outputId": "0823a218-a697-4f54-9238-f709af26b50c"
      },
      "source": [
        "# Evaluate the model 6 \n",
        "model_6.evaluate(X_test , y_test)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 978us/step - loss: 0.0735\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07352113723754883"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqlPT_dXfuQf"
      },
      "source": [
        "### 2. Get the most up to date data on Bitcoin, train a model & see how it goes (our data goes up to May 18 2021)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUKI8td_b_F_",
        "outputId": "e354aba8-e55e-4369-c992-da2469fc7ef9"
      },
      "source": [
        "# Loading the in the latest csv from Coindesk\n",
        "df_updated = pd.read_csv('/content/BTC_USD_2014-11-02_2021-09-09-CoinDesk.csv' , \n",
        "                 parse_dates = ['Date'] , \n",
        "                 index_col = ['Date'])\n",
        "\n",
        "bitcoin_prices_updated = pd.DataFrame(df_updated[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\n",
        "bitcoin_prices_updated.head(10) , bitcoin_prices_updated.shape"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                Price\n",
              " Date                 \n",
              " 2014-11-02  325.22633\n",
              " 2014-11-03  331.60083\n",
              " 2014-11-04  324.71833\n",
              " 2014-11-05  332.45666\n",
              " 2014-11-06  336.58500\n",
              " 2014-11-07  346.77500\n",
              " 2014-11-08  344.81166\n",
              " 2014-11-09  343.06500\n",
              " 2014-11-10  358.50166\n",
              " 2014-11-11  368.07666, (2503, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52OylhbEiIII"
      },
      "source": [
        "prices_updated = bitcoin_prices_updated['Price'].to_numpy()"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZYgYStegy7E"
      },
      "source": [
        "def make_windows(x, window_size=7, horizon=1):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
        "  \"\"\"\n",
        "  \n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "  return windows, labels"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzAx_9nhfGF",
        "outputId": "fd57be42-7351-4f55-9681-b8210d54f2da"
      },
      "source": [
        "full_windows , full_labels = make_windows(prices_updated)\n",
        "len(full_windows), len(full_labels)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2496, 2496)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ5Iir7jvLeY",
        "outputId": "726bcf5a-900a-470e-e664-3a4ffc59a440"
      },
      "source": [
        "# Looking at few examples of how price is scaled\n",
        "for i in range(3):\n",
        "  print(f'Window: {full_windows[i]} --> Label {full_labels[i]}')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window: [325.22633 331.60083 324.71833 332.45666 336.585   346.775   344.81166] --> Label [343.065]\n",
            "Window: [331.60083 324.71833 332.45666 336.585   346.775   344.81166 343.065  ] --> Label [358.50166]\n",
            "Window: [324.71833 332.45666 336.585   346.775   344.81166 343.065   358.50166] --> Label [368.07666]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HCIuZdxh-GC",
        "outputId": "80ec4440-c36b-419b-93a4-bdf1f8b0de5f"
      },
      "source": [
        "# Making train and test splits\n",
        "train_windows ,  test_windows ,train_labels,  test_labels =  make_train_test_splits(full_windows , full_labels)\n",
        "\n",
        "len(train_windows) ,  len(test_windows) , len(train_labels),  len(test_labels) "
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1996, 500, 1996, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Ve_RGCm5Ez"
      },
      "source": [
        "Now we're building the same Model 1 with the new coindesk data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IytoXshkhJE",
        "outputId": "ef32194c-116c-457c-b3ba-1ad5fec14841"
      },
      "source": [
        "# Building the Model 1 with the updated data\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Construct the model \n",
        "model_1 = tf.keras.Sequential([\n",
        "  layers.Dense(128, activation= 'relu') ,\n",
        "  layers.Dense(HORIZON , activation = 'linear')\n",
        "])\n",
        "\n",
        "# Compiling the model \n",
        "model_1.compile(loss = 'mae' , \n",
        "                optimizer = tf.keras.optimizers.Adam() , \n",
        "                metrics = ['mae'])\n",
        "\n",
        "# Fit the model \n",
        "model_1.fit(x = train_windows , \n",
        "            y = train_labels , \n",
        "            epochs = 100 , batch_size = 128 , verbose = 0 , \n",
        "            validation_data = (test_windows , test_labels))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5111c4be50>"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq-4_z0rm_Gc",
        "outputId": "9ef52497-94a5-4690-fdda-f9cb9a2cd0de"
      },
      "source": [
        "# Evaluating the model \n",
        "model_1.evaluate(test_windows , test_labels)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 1ms/step - loss: 884.0141 - mae: 884.0141\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[884.0140991210938, 884.0140991210938]"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIuiLp-PnYlS"
      },
      "source": [
        "### 3. For most of our models we used WINDOW_SIZE=7, but is there a better window size?\n",
        "\n",
        "* Setup a series of experiments to find whether or not thereâ€™s a better window size.\n",
        "* For example, you might train 10 different models with HORIZON=1 but with window sizes ranging from 2-12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P08KPQgG6op2"
      },
      "source": [
        "# Writing a evaluation function based on the preds and targets \n",
        "def evaluate_preds(y_true , y_pred):\n",
        "\n",
        "  # Casting the values to float32 \n",
        "  y_true = tf.cast(y_true , tf.float32)\n",
        "  y_pred = tf.cast(y_pred , tf.float32)\n",
        "\n",
        "\n",
        "  # Calculate the metrics \n",
        "  mae = tf.keras.metrics.mean_absolute_error(y_true , y_pred)\n",
        "  mse = tf.keras.metrics.mean_squared_error(y_true , y_pred)\n",
        "  rmse = tf.sqrt(mse)\n",
        "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true , y_pred)\n",
        "  \n",
        "  # For longer horizons \n",
        "  if mae.ndim > 0:\n",
        "    mae = tf.reduce_sum(mae)\n",
        "    mse = tf.reduce_sum(mse)\n",
        "    rmse = tf.reduce_sum(rmse)\n",
        "    mape = tf.reduce_sum(mape)\n",
        "\n",
        "  return {'mae' : mae.numpy() , \n",
        "          'mse': mse.numpy() , \n",
        "          'rmse': rmse.numpy() , \n",
        "          'mape': mape.numpy() }"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x11lEVED6d9A",
        "outputId": "513efdb8-1aea-4b98-e3b3-a9a2ecac82a4"
      },
      "source": [
        "# Writing a for loop to iterate over the Window size and build 10 different models\n",
        "\n",
        "# 10 Different models with window size ranging from (2 - 12) and store the results\n",
        "model_results_list = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "for size in tqdm(range(2,12)):\n",
        "  HORIZON = 1 \n",
        "  WINDOW_SIZE = size\n",
        "\n",
        "  # Making window and labels \n",
        "  full_windows , full_labels = make_windows(prices, window_size= WINDOW_SIZE , horizon= HORIZON)\n",
        "  \n",
        "\n",
        "  # Splitting the data in train and test\n",
        "  train_windows ,  test_windows ,train_labels,  test_labels = make_train_test_splits(full_windows , full_labels)\n",
        "\n",
        "\n",
        "  # Building a simple dense model\n",
        "  input = layers.Input(shape = (WINDOW_SIZE ,) , name = 'Input_layer')\n",
        "  x = layers.Dense(128 , activation= 'relu')(input)\n",
        "  output = layers.Dense(HORIZON , activation= 'linear')(x)\n",
        "\n",
        "  # Packing into a model \n",
        "  model = tf.keras.Model(input , output , name = f'model_windowed_{size}')\n",
        "\n",
        "  # Compiling and fitting the model \n",
        "  model.compile(loss = 'mae' , optimizer = 'adam' , metrics = 'mae')\n",
        "\n",
        "  model.fit(train_windows , train_labels , \n",
        "            epochs = 100 , verbose = 0 , \n",
        "            batch_size = 128 , \n",
        "            validation_data = (test_windows , test_labels))\n",
        "  \n",
        "\n",
        "  # Making predictions \n",
        "  preds_ = model.predict(test_windows)\n",
        "  y_preds = tf.squeeze(preds_)\n",
        "\n",
        "  results = evaluate_preds(tf.squeeze(test_labels) , y_preds)\n",
        "  model_results_list.append(results)\n",
        " "
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.34s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyp-2gqoUipn",
        "outputId": "2203ccf7-83d5-4d69-bcbd-39959c2d8ccf"
      },
      "source": [
        "# Below are the 10 different models result \n",
        "model_results_list"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'mae': 569.15063, 'mape': 2.527791, 'mse': 1159396.8, 'rmse': 1076.7529},\n",
              " {'mae': 565.61005, 'mape': 2.5274613, 'mse': 1139677.9, 'rmse': 1067.557},\n",
              " {'mae': 623.4455, 'mape': 2.8342047, 'mse': 1266272.4, 'rmse': 1125.2877},\n",
              " {'mae': 565.611, 'mape': 2.5193882, 'mse': 1157947.0, 'rmse': 1076.0795},\n",
              " {'mae': 575.82715, 'mape': 2.5713227, 'mse': 1181491.9, 'rmse': 1086.9645},\n",
              " {'mae': 624.6512, 'mape': 2.8559427, 'mse': 1279309.4, 'rmse': 1131.0656},\n",
              " {'mae': 673.8467, 'mape': 3.155991, 'mse': 1400997.6, 'rmse': 1183.6375},\n",
              " {'mae': 661.0687, 'mape': 3.0541995, 'mse': 1343185.1, 'rmse': 1158.9586},\n",
              " {'mae': 583.37537, 'mape': 2.6407204, 'mse': 1204248.0, 'rmse': 1097.3823},\n",
              " {'mae': 701.847, 'mape': 3.2888002, 'mse': 1443913.1, 'rmse': 1201.6294}]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzH981vjL-fT"
      },
      "source": [
        "### 4. Create a windowed dataset just like the ones we used for model_1 using  [tf.keras.preprocessing.timeseries_dataset_from_array()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)  and retrain model_1 using the recreated dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcRI4RcIYnYE"
      },
      "source": [
        "WINDOW_SIZE = 7 \n",
        "HORIZON = 1"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zakDAvtCY32O"
      },
      "source": [
        "# Make the splits \n",
        "def make_train_test_splits(windows , labels , test_split = 0.2):\n",
        "  split_size = int(len(windows) * (1 - test_split))\n",
        "  train_windows = windows[:split_size]\n",
        "  train_labels = labels[:split_size]\n",
        "  test_windows = windows[split_size:]\n",
        "  test_labels = labels[split_size:]\n",
        "\n",
        "  return train_windows ,  test_windows ,train_labels,  test_labels"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujv0D7izXNhn"
      },
      "source": [
        "ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data = prices , targets = prices , sequence_length = WINDOW_SIZE , sequence_stride = HORIZON, \n",
        "    batch_size = 128\n",
        ")"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5sXVH-ctoF"
      },
      "source": [
        "train_size , test_size = int(0.8 * len(ds)) ,int(0.2 * len(ds))"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlzXwk84cxp6"
      },
      "source": [
        "train_ds = ds.take(train_size)\n",
        "test_ds = ds.skip(train_size).take(test_size)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcTn-l1Idl3t",
        "outputId": "8c237383-78e4-41c1-fed4-cc394976b141"
      },
      "source": [
        "for x , y in train_ds.take(1):\n",
        "  print(x[:2] , y[:2])"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[123.65499 125.455   108.58483 118.67466 121.33866 120.65533 121.795  ]\n",
            " [125.455   108.58483 118.67466 121.33866 120.65533 121.795   123.033  ]], shape=(2, 7), dtype=float64) tf.Tensor([123.65499 125.455  ], shape=(2,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuP5_htZduAu",
        "outputId": "7acba072-6059-4653-8769-7671b0b37fa4"
      },
      "source": [
        "for x , y in test_ds.take(1):\n",
        "  print(x[:2] , y[:2])"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[10302.19071368 10301.65965169 10231.42151196 10168.28770938\n",
            "  10223.5055788  10138.33520522  9984.52051597]\n",
            " [10301.65965169 10231.42151196 10168.28770938 10223.5055788\n",
            "  10138.33520522  9984.52051597 10031.86670899]], shape=(2, 7), dtype=float64) tf.Tensor([10302.19071368 10301.65965169], shape=(2,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieBAnXs0eJAy",
        "outputId": "a9f8cb6d-e959-4b31-effb-2840c3960866"
      },
      "source": [
        "# Building the Model 1 with the updated data\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Building a simple dense model\n",
        "input = layers.Input(shape = (WINDOW_SIZE ,) , name = 'Input_layer' , dtype = tf.float32)\n",
        "x = layers.Dense(128 , activation= 'relu')(input)\n",
        "output = layers.Dense(HORIZON , activation= 'linear')(x)\n",
        "\n",
        "# Packing into a model \n",
        "model = tf.keras.Model(input , output)\n",
        "\n",
        "# Compiling the model \n",
        "model.compile(loss = 'mae' , \n",
        "                optimizer = tf.keras.optimizers.Adam() , \n",
        "                metrics = ['mae'])\n",
        "\n",
        "# Fit the model \n",
        "model.fit(train_ds ,\n",
        "          epochs = 100 , verbose = 0 , \n",
        "            validation_data = test_ds)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51a9f27290>"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPBtfK2diAkV",
        "outputId": "31613fb9-493c-4218-dee5-112b318daa9a"
      },
      "source": [
        "# Evaluating the model on the test set\n",
        "model.evaluate(test_ds)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 20ms/step - loss: 428.2230 - mae: 428.2230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[428.22296142578125, 428.22296142578125]"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GONHDDlhwKr"
      },
      "source": [
        "### 5. For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.\n",
        "\n",
        "  * Are there any other features you think you could add?\n",
        "  * If so, try it out, how do these affect the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA-51-MMxzNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "e7b2007e-d7c4-4634-e7bf-25cad0149888"
      },
      "source": [
        "df"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Currency</th>\n",
              "      <th>Closing Price (USD)</th>\n",
              "      <th>24h Open (USD)</th>\n",
              "      <th>24h High (USD)</th>\n",
              "      <th>24h Low (USD)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>BTC</td>\n",
              "      <td>123.654990</td>\n",
              "      <td>124.304660</td>\n",
              "      <td>124.751660</td>\n",
              "      <td>122.563490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>BTC</td>\n",
              "      <td>125.455000</td>\n",
              "      <td>123.654990</td>\n",
              "      <td>125.758500</td>\n",
              "      <td>123.633830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>BTC</td>\n",
              "      <td>108.584830</td>\n",
              "      <td>125.455000</td>\n",
              "      <td>125.665660</td>\n",
              "      <td>83.328330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>BTC</td>\n",
              "      <td>118.674660</td>\n",
              "      <td>108.584830</td>\n",
              "      <td>118.675000</td>\n",
              "      <td>107.058160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>BTC</td>\n",
              "      <td>121.338660</td>\n",
              "      <td>118.674660</td>\n",
              "      <td>121.936330</td>\n",
              "      <td>118.005660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-05-14</th>\n",
              "      <td>BTC</td>\n",
              "      <td>49764.132082</td>\n",
              "      <td>49596.778891</td>\n",
              "      <td>51448.798576</td>\n",
              "      <td>46294.720180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-05-15</th>\n",
              "      <td>BTC</td>\n",
              "      <td>50032.693137</td>\n",
              "      <td>49717.354353</td>\n",
              "      <td>51578.312545</td>\n",
              "      <td>48944.346536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-05-16</th>\n",
              "      <td>BTC</td>\n",
              "      <td>47885.625255</td>\n",
              "      <td>49926.035067</td>\n",
              "      <td>50690.802950</td>\n",
              "      <td>47005.102292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-05-17</th>\n",
              "      <td>BTC</td>\n",
              "      <td>45604.615754</td>\n",
              "      <td>46805.537852</td>\n",
              "      <td>49670.414174</td>\n",
              "      <td>43868.638969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-05-18</th>\n",
              "      <td>BTC</td>\n",
              "      <td>43144.471291</td>\n",
              "      <td>46439.336570</td>\n",
              "      <td>46622.853437</td>\n",
              "      <td>42102.346430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2787 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Currency  Closing Price (USD)  ...  24h High (USD)  24h Low (USD)\n",
              "Date                                      ...                               \n",
              "2013-10-01      BTC           123.654990  ...      124.751660     122.563490\n",
              "2013-10-02      BTC           125.455000  ...      125.758500     123.633830\n",
              "2013-10-03      BTC           108.584830  ...      125.665660      83.328330\n",
              "2013-10-04      BTC           118.674660  ...      118.675000     107.058160\n",
              "2013-10-05      BTC           121.338660  ...      121.936330     118.005660\n",
              "...             ...                  ...  ...             ...            ...\n",
              "2021-05-14      BTC         49764.132082  ...    51448.798576   46294.720180\n",
              "2021-05-15      BTC         50032.693137  ...    51578.312545   48944.346536\n",
              "2021-05-16      BTC         47885.625255  ...    50690.802950   47005.102292\n",
              "2021-05-17      BTC         45604.615754  ...    49670.414174   43868.638969\n",
              "2021-05-18      BTC         43144.471291  ...    46622.853437   42102.346430\n",
              "\n",
              "[2787 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMRA5-nVmPnh"
      },
      "source": [
        "import datetime "
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "VtYlinvCm8EQ",
        "outputId": "42eb88cb-03e8-45c5-9d32-34f1ac6d4831"
      },
      "source": [
        "# Creating a day of week feature \n",
        "df['day_of_week'] = df.index.dayofweek\n",
        "df.head(10)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Currency</th>\n",
              "      <th>Closing Price (USD)</th>\n",
              "      <th>24h Open (USD)</th>\n",
              "      <th>24h High (USD)</th>\n",
              "      <th>24h Low (USD)</th>\n",
              "      <th>day_of_week</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>BTC</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>124.30466</td>\n",
              "      <td>124.75166</td>\n",
              "      <td>122.56349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>BTC</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>123.65499</td>\n",
              "      <td>125.75850</td>\n",
              "      <td>123.63383</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>BTC</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>125.45500</td>\n",
              "      <td>125.66566</td>\n",
              "      <td>83.32833</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>BTC</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>108.58483</td>\n",
              "      <td>118.67500</td>\n",
              "      <td>107.05816</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>BTC</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>118.67466</td>\n",
              "      <td>121.93633</td>\n",
              "      <td>118.00566</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-06</th>\n",
              "      <td>BTC</td>\n",
              "      <td>120.65533</td>\n",
              "      <td>121.33866</td>\n",
              "      <td>121.85216</td>\n",
              "      <td>120.55450</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-07</th>\n",
              "      <td>BTC</td>\n",
              "      <td>121.79500</td>\n",
              "      <td>120.65533</td>\n",
              "      <td>121.99166</td>\n",
              "      <td>120.43199</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-08</th>\n",
              "      <td>BTC</td>\n",
              "      <td>123.03300</td>\n",
              "      <td>121.79500</td>\n",
              "      <td>123.64016</td>\n",
              "      <td>121.35066</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-09</th>\n",
              "      <td>BTC</td>\n",
              "      <td>124.04900</td>\n",
              "      <td>123.03300</td>\n",
              "      <td>124.78350</td>\n",
              "      <td>122.59266</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-10</th>\n",
              "      <td>BTC</td>\n",
              "      <td>125.96116</td>\n",
              "      <td>124.04900</td>\n",
              "      <td>128.01683</td>\n",
              "      <td>123.81966</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Currency  Closing Price (USD)  ...  24h Low (USD)  day_of_week\n",
              "Date                                      ...                            \n",
              "2013-10-01      BTC            123.65499  ...      122.56349            1\n",
              "2013-10-02      BTC            125.45500  ...      123.63383            2\n",
              "2013-10-03      BTC            108.58483  ...       83.32833            3\n",
              "2013-10-04      BTC            118.67466  ...      107.05816            4\n",
              "2013-10-05      BTC            121.33866  ...      118.00566            5\n",
              "2013-10-06      BTC            120.65533  ...      120.55450            6\n",
              "2013-10-07      BTC            121.79500  ...      120.43199            0\n",
              "2013-10-08      BTC            123.03300  ...      121.35066            1\n",
              "2013-10-09      BTC            124.04900  ...      122.59266            2\n",
              "2013-10-10      BTC            125.96116  ...      123.81966            3\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppVyHOd9pZUC"
      },
      "source": [
        "# Defining the hyper parameters \n",
        "HORIZON = 1 \n",
        "WINDOW_SIZE = 7 \n",
        "\n",
        "bitcoin_prices_windowed['day_of_week'] = bitcoin_prices_windowed.index.dayofweek"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6zN_LcoqCca"
      },
      "source": [
        "# Getting three kinds of data (univariate , multivariate and the day of week)\n",
        "\n",
        "# Univariate data \n",
        "full_windows , full_labels = make_windows_scaled(prices)\n",
        "train_windows , test_windows , train_labels , test_labels = make_train_test_splits(full_windows , full_labels)\n",
        "\n",
        "# Multivaritate dat \n",
        "X = bitcoin_prices_windowed.dropna().drop('Price' , axis = 1).astype(np.float32)\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y = bitcoin_prices_windowed.dropna()['Price'].astype(np.float32)\n",
        "\n",
        "# Day of week \n",
        "day_of_week = bitcoin_prices_windowed.dropna()['day_of_week'].to_list()"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YCJnEBEzj69",
        "outputId": "898f8ea3-7283-4d0c-9582-cc0be755531d"
      },
      "source": [
        "# Checking the shapes \n",
        "print(full_windows.shape , full_labels.shape)\n",
        "print(X.shape , y.shape)\n",
        "print(len(day_of_week))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2780, 7) (2780, 1)\n",
            "(2780, 9) (2780,)\n",
            "2780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvJBEkZR4XVp",
        "outputId": "abfcb196-424a-4e88-e361-196fbe4a9d94"
      },
      "source": [
        "# Splitting the multivariate and the day_of_week to train and test splits \n",
        "split_size = int(len(X) * 0.8)\n",
        "train_block_rewards , test_block_rewards = X[:split_size] , X[split_size:]\n",
        "train_days , test_days = day_of_week[:split_size] , day_of_week[split_size:]\n",
        " \n",
        "len(train_block_rewards), len(train_days) , len(test_block_rewards) , len(test_days)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2224, 2224, 556, 556)"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQd1NpWSzke0",
        "outputId": "6519417a-f08d-4fa4-dc19-20e64cbfc51a"
      },
      "source": [
        "# Building a performant dataset for train and test \n",
        "\n",
        "train_data_tribid = tf.data.Dataset.from_tensor_slices((train_windows , \n",
        "                                                        train_block_rewards , \n",
        "                                                        train_days))\n",
        "\n",
        "train_labels_tribid = tf.data.Dataset.from_tensor_slices(train_labels)\n",
        "\n",
        "# The test/val split \n",
        "test_data_tribid = tf.data.Dataset.from_tensor_slices((test_windows , \n",
        "                                                       test_block_rewards , \n",
        "                                                       test_days))\n",
        "\n",
        "test_labels_tribid = tf.data.Dataset.from_tensor_slices(test_labels)\n",
        "\n",
        "# Zipping the data and labels into one complete dataset \n",
        "tribid_train_ds = tf.data.Dataset.zip((train_data_tribid , train_labels_tribid))\n",
        "tribid_test_ds = tf.data.Dataset.zip((test_data_tribid , test_labels_tribid))\n",
        "\n",
        "# Applying prefetch and batching the dataset \n",
        "tribid_train_ds = tribid_train_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "tribid_test_ds = tribid_test_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "tribid_train_ds ,tribid_test_ds"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PrefetchDataset shapes: (((None, 7), (None, 9), (None,)), (None, 1)), types: ((tf.float64, tf.float32, tf.int32), tf.float64)>,\n",
              " <PrefetchDataset shapes: (((None, 7), (None, 9), (None,)), (None, 1)), types: ((tf.float64, tf.float32, tf.int32), tf.float64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyhukkUSzmu8",
        "outputId": "ad33049b-ef97-4c4a-d5fc-ce6ab15f50a9"
      },
      "source": [
        "# Building a tribid model \n",
        "\n",
        "input_windows = layers.Input(shape = (7,) , dtype=tf.float64 , name='Window Inputs')\n",
        "exp_layer_1 = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(input_windows)\n",
        "conv1 = layers.Conv1D(filters= 32 , kernel_size=5 , padding='causal' , activation= 'relu')(exp_layer_1)\n",
        "window_model = tf.keras.Model(input_windows , conv1 , name = 'Windowed model')\n",
        "\n",
        "input_blocks = layers.Input(shape = (9,) , dtype= tf.float32 , name ='Block rewards input')\n",
        "exp_layer_2 = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(input_blocks)\n",
        "conv2 = layers.Conv1D(filters = 32 , kernel_size= 5 , activation= 'relu' , padding = 'causal')(exp_layer_2)\n",
        "block_model = tf.keras.Model(input_blocks , conv2 , name = 'Block rewards model')\n",
        "\n",
        "\n",
        "# Use expand dims to match the same shape output (None , 1 , 128)\n",
        "# whereas without expand dims it would be (None , 128)\n",
        "input_days = layers.Input(shape= (1,) , dtype = tf.int32 , name ='Days of week Input')\n",
        "exp_layer_3 = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(input_days)\n",
        "dense = layers.Dense(128 , activation= 'relu')(exp_layer_3)\n",
        "days_model = tf.keras.Model(input_days , dense , name = 'Days Model')\n",
        "\n",
        "# Concatenating the inputs \n",
        "concat = layers.Concatenate(name = 'combined_outputs' )([window_model.output , \n",
        "                                                           block_model.output , \n",
        "                                                           days_model.output])\n",
        "\n",
        "# Creating the output layer \n",
        "dropout = layers.Dropout(0.4)(concat)\n",
        "output_layer = layers.Dense(1 , activation = 'linear')(dropout)\n",
        "\n",
        "# Putting everything into a model \n",
        "tribid_model = tf.keras.Model(inputs = [window_model.input , \n",
        "                                        block_model.input , \n",
        "                                        days_model.input] , \n",
        "                              outputs = output_layer)\n",
        "tribid_model.summary()\n",
        "\n"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Window Inputs (InputLayer)      [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Block rewards input (InputLayer [(None, 9)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Days of week Input (InputLayer) [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 1, 7)         0           Window Inputs[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 1, 9)         0           Block rewards input[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 1, 1)         0           Days of week Input[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1, 32)        1152        lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 1, 32)        1472        lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_103 (Dense)               (None, 1, 128)       256         lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "combined_outputs (Concatenate)  (None, 1, 192)       0           conv1d_2[0][0]                   \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "                                                                 dense_103[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1, 192)       0           combined_outputs[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_104 (Dense)               (None, 1, 1)         193         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 3,073\n",
            "Trainable params: 3,073\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDB8vFtu17Rp",
        "outputId": "74223741-7d90-461c-f146-97f9b3055d0c"
      },
      "source": [
        "# Compiling and fitting the model \n",
        "tribid_model.compile(loss = 'mae' , \n",
        "                     optimizer = 'adam' , metrics = ['mae'])\n",
        "\n",
        "# Fitting the model \n",
        "tribid_model.fit(tribid_train_ds , \n",
        "                 epochs = 20,  \n",
        "                 validation_data = tribid_test_ds , verbose = 2)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "18/18 - 1s - loss: 173.1271 - mae: 173.1271 - val_loss: 124.0174 - val_mae: 124.0174\n",
            "Epoch 2/20\n",
            "18/18 - 0s - loss: 96.4324 - mae: 96.4324 - val_loss: 37.3842 - val_mae: 37.3842\n",
            "Epoch 3/20\n",
            "18/18 - 0s - loss: 58.4491 - mae: 58.4491 - val_loss: 40.1555 - val_mae: 40.1555\n",
            "Epoch 4/20\n",
            "18/18 - 0s - loss: 25.2752 - mae: 25.2752 - val_loss: 25.2996 - val_mae: 25.2996\n",
            "Epoch 5/20\n",
            "18/18 - 0s - loss: 6.4564 - mae: 6.4564 - val_loss: 7.1498 - val_mae: 7.1498\n",
            "Epoch 6/20\n",
            "18/18 - 0s - loss: 3.2152 - mae: 3.2152 - val_loss: 14.4171 - val_mae: 14.4171\n",
            "Epoch 7/20\n",
            "18/18 - 0s - loss: 1.3200 - mae: 1.3200 - val_loss: 5.2407 - val_mae: 5.2407\n",
            "Epoch 8/20\n",
            "18/18 - 0s - loss: 0.3744 - mae: 0.3744 - val_loss: 0.3289 - val_mae: 0.3289\n",
            "Epoch 9/20\n",
            "18/18 - 0s - loss: 0.2766 - mae: 0.2766 - val_loss: 0.3679 - val_mae: 0.3679\n",
            "Epoch 10/20\n",
            "18/18 - 0s - loss: 0.2502 - mae: 0.2502 - val_loss: 0.8294 - val_mae: 0.8294\n",
            "Epoch 11/20\n",
            "18/18 - 0s - loss: 0.4526 - mae: 0.4526 - val_loss: 1.4310 - val_mae: 1.4310\n",
            "Epoch 12/20\n",
            "18/18 - 0s - loss: 0.2831 - mae: 0.2831 - val_loss: 0.7019 - val_mae: 0.7019\n",
            "Epoch 13/20\n",
            "18/18 - 0s - loss: 0.2143 - mae: 0.2143 - val_loss: 0.2403 - val_mae: 0.2403\n",
            "Epoch 14/20\n",
            "18/18 - 0s - loss: 0.1896 - mae: 0.1896 - val_loss: 0.2570 - val_mae: 0.2570\n",
            "Epoch 15/20\n",
            "18/18 - 0s - loss: 0.1637 - mae: 0.1637 - val_loss: 0.2061 - val_mae: 0.2061\n",
            "Epoch 16/20\n",
            "18/18 - 0s - loss: 0.1283 - mae: 0.1283 - val_loss: 0.2460 - val_mae: 0.2460\n",
            "Epoch 17/20\n",
            "18/18 - 0s - loss: 0.1077 - mae: 0.1077 - val_loss: 0.1839 - val_mae: 0.1839\n",
            "Epoch 18/20\n",
            "18/18 - 0s - loss: 0.1213 - mae: 0.1213 - val_loss: 0.2423 - val_mae: 0.2423\n",
            "Epoch 19/20\n",
            "18/18 - 0s - loss: 0.0994 - mae: 0.0994 - val_loss: 0.2242 - val_mae: 0.2242\n",
            "Epoch 20/20\n",
            "18/18 - 0s - loss: 0.1007 - mae: 0.1007 - val_loss: 0.1563 - val_mae: 0.1563\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51aa476510>"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzCKgn8598cB",
        "outputId": "cefb2a43-c5c7-4e88-c02f-952c57239768"
      },
      "source": [
        "# Evaluating the model \n",
        "tribid_model.evaluate(tribid_test_ds)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1563 - mae: 0.1563\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.15634702146053314, 0.15634702146053314]"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaqbrn-xnnjW"
      },
      "source": [
        "### 6. Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for model_8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGAuQZpovQ9"
      },
      "source": [
        "**Things to do**\n",
        "- Train an ensemble model on the whole data. \n",
        "- Make one dataset (no test/valid) which will use to predict future forecasts of bitcoins. \n",
        "- Make a function that will take the number of iterations and different loss functions to train the model with. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS_NBQuqt5Bb",
        "outputId": "457d54dd-b86b-4fe8-f95b-64b07999e3b4"
      },
      "source": [
        "# Make one whole dataset (with the updated bitcoin prices 2014 - 2021)\n",
        "\n",
        "X_all = bitcoin_prices_windowed.drop(['Price' , 'block_reward' , 'day_of_week'] , axis = 1).dropna().to_numpy()\n",
        "y_all = bitcoin_prices_windowed.dropna()['Price'].to_numpy()\n",
        "\n",
        "whole_ds = tf.data.Dataset.from_tensor_slices((X_all , y_all))\n",
        "whole_ds = whole_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "whole_ds"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)>"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5zZCOdSvBDX"
      },
      "source": [
        "# Creating the function \n",
        "\n",
        "def get_ensemble_models(horizon = HORIZON , \n",
        "                        dataset = whole_ds , \n",
        "                        num_iter = 10 , \n",
        "                        num_epochs = 100 , \n",
        "                        loss_fns = ['mae' , 'mse' , 'mape']):\n",
        "  \n",
        "\n",
        "  # Make a empty list of the ensemble models \n",
        "  ensemble_models = []\n",
        "\n",
        "  # Create num_iter number of models per loss functions \n",
        "  for i in range(num_iter):\n",
        "    for loss_functions in loss_fns:\n",
        "      print(f'Optimizing model by reducing: {loss_functions} for {num_epochs} epochs, model number: {i}')\n",
        "\n",
        "      model = tf.keras.Sequential([\n",
        "          layers.Dense(128 , kernel_initializer='he_normal' , activation= 'relu'),\n",
        "          layers.Dense(128 , kernel_initializer= 'he_normal', activation= 'relu'),\n",
        "          layers.Dense(HORIZON)\n",
        "      ])\n",
        "\n",
        "      # Compiling the model \n",
        "      model.compile(loss = loss_functions , \n",
        "                    optimizer = 'adam' , metrics = ['mae' , 'mse'])\n",
        "      \n",
        "      # Fit the model \n",
        "      model.fit(dataset , \n",
        "                epochs = num_epochs , \n",
        "                verbose = 0,\n",
        "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
        "                                                            patience=200,\n",
        "                                                            restore_best_weights=True),\n",
        "                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",\n",
        "                                                                patience=100,\n",
        "                                                                verbose=1)])\n",
        "      \n",
        "      ensemble_models.append(model)\n",
        "\n",
        "  return ensemble_models"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btGEQNfLvCXL",
        "outputId": "ff374850-758c-4f44-d9c0-fdfd86416334"
      },
      "source": [
        "# Running the above function \n",
        "ensemble_models = get_ensemble_models(num_iter=5 , num_epochs= 1000 , horizon = 1)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing model by reducing: mae for 1000 epochs, model number: 0\n",
            "\n",
            "Epoch 00215: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00418: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 0\n",
            "\n",
            "Epoch 00138: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00422: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 0\n",
            "\n",
            "Epoch 00259: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00359: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 1\n",
            "\n",
            "Epoch 00269: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00386: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 1\n",
            "\n",
            "Epoch 00337: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00664: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 1\n",
            "\n",
            "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00251: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00468: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 2\n",
            "\n",
            "Epoch 00292: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00412: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 2\n",
            "\n",
            "Epoch 00173: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00620: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 2\n",
            "\n",
            "Epoch 00214: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00320: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00421: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 3\n",
            "\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00258: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 3\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00432: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 3\n",
            "\n",
            "Epoch 00218: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00318: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 4\n",
            "\n",
            "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00360: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 4\n",
            "\n",
            "Epoch 00251: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00527: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 4\n",
            "\n",
            "Epoch 00277: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00379: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00505: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM4bW2EN__jH"
      },
      "source": [
        "# Making future forecastts of Bitcoins (using the whole data)\n",
        "def make_future_forecast(values , model_list , into_future , window_size):\n",
        "\n",
        "  future_forecast = []\n",
        "  last_window = values[-window_size:]\n",
        "\n",
        "  for _ in range(into_future): \n",
        "    for model in model_list:\n",
        "    \n",
        "      future_pred = model.predict(tf.expand_dims(last_window, axis= 0))\n",
        "      #future_pred = model.predict(last_window)\n",
        "      print(f'Predicing on: \\n {last_window} --> Prediction: {tf.squeeze(future_pred).numpy()}\\n')\n",
        "\n",
        "      future_forecast.append(tf.squeeze(future_pred).numpy())\n",
        "\n",
        "      # Update the last window \n",
        "      last_window = np.append(last_window , future_pred)[-window_size:]\n",
        "  return future_forecast"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG-C8sGguG-o",
        "outputId": "572c7859-d3fb-4277-92c8-32653abe568d"
      },
      "source": [
        "# Getting the future forecast \n",
        "future_forecast = make_future_forecast(y_all , ensemble_models , into_future= 14 , window_size = 7 )"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicing on: \n",
            " [56573.5554719  52147.82118698 49764.1320816  50032.69313676\n",
            " 47885.62525472 45604.61575361 43144.47129086] --> Prediction: 56175.19921875\n",
            "\n",
            "Predicing on: \n",
            " [52147.82118698 49764.1320816  50032.69313676 47885.62525472\n",
            " 45604.61575361 43144.47129086 56175.19921875] --> Prediction: 53977.78125\n",
            "\n",
            "Predicing on: \n",
            " [49764.1320816  50032.69313676 47885.62525472 45604.61575361\n",
            " 43144.47129086 56175.19921875 53977.78125   ] --> Prediction: 48286.578125\n",
            "\n",
            "Predicing on: \n",
            " [50032.69313676 47885.62525472 45604.61575361 43144.47129086\n",
            " 56175.19921875 53977.78125    48286.578125  ] --> Prediction: 49753.5859375\n",
            "\n",
            "Predicing on: \n",
            " [47885.62525472 45604.61575361 43144.47129086 56175.19921875\n",
            " 53977.78125    48286.578125   49753.5859375 ] --> Prediction: 45806.8046875\n",
            "\n",
            "Predicing on: \n",
            " [45604.61575361 43144.47129086 56175.19921875 53977.78125\n",
            " 48286.578125   49753.5859375  45806.8046875 ] --> Prediction: 45975.1015625\n",
            "\n",
            "Predicing on: \n",
            " [43144.47129086 56175.19921875 53977.78125    48286.578125\n",
            " 49753.5859375  45806.8046875  45975.1015625 ] --> Prediction: 45759.26953125\n",
            "\n",
            "Predicing on: \n",
            " [56175.19921875 53977.78125    48286.578125   49753.5859375\n",
            " 45806.8046875  45975.1015625  45759.26953125] --> Prediction: 56390.80859375\n",
            "\n",
            "Predicing on: \n",
            " [53977.78125    48286.578125   49753.5859375  45806.8046875\n",
            " 45975.1015625  45759.26953125 56390.80859375] --> Prediction: 51704.2265625\n",
            "\n",
            "Predicing on: \n",
            " [48286.578125   49753.5859375  45806.8046875  45975.1015625\n",
            " 45759.26953125 56390.80859375 51704.2265625 ] --> Prediction: 48911.22265625\n",
            "\n",
            "Predicing on: \n",
            " [49753.5859375  45806.8046875  45975.1015625  45759.26953125\n",
            " 56390.80859375 51704.2265625  48911.22265625] --> Prediction: 49763.04296875\n",
            "\n",
            "Predicing on: \n",
            " [45806.8046875  45975.1015625  45759.26953125 56390.80859375\n",
            " 51704.2265625  48911.22265625 49763.04296875] --> Prediction: 45226.55078125\n",
            "\n",
            "Predicing on: \n",
            " [45975.1015625  45759.26953125 56390.80859375 51704.2265625\n",
            " 48911.22265625 49763.04296875 45226.55078125] --> Prediction: 46871.37109375\n",
            "\n",
            "Predicing on: \n",
            " [45759.26953125 56390.80859375 51704.2265625  48911.22265625\n",
            " 49763.04296875 45226.55078125 46871.37109375] --> Prediction: 48359.44921875\n",
            "\n",
            "Predicing on: \n",
            " [56390.80859375 51704.2265625  48911.22265625 49763.04296875\n",
            " 45226.55078125 46871.37109375 48359.44921875] --> Prediction: 56190.0625\n",
            "\n",
            "Predicing on: \n",
            " [51704.2265625  48911.22265625 49763.04296875 45226.55078125\n",
            " 46871.37109375 48359.44921875 56190.0625    ] --> Prediction: 50447.91796875\n",
            "\n",
            "Predicing on: \n",
            " [48911.22265625 49763.04296875 45226.55078125 46871.37109375\n",
            " 48359.44921875 56190.0625     50447.91796875] --> Prediction: 47680.9375\n",
            "\n",
            "Predicing on: \n",
            " [49763.04296875 45226.55078125 46871.37109375 48359.44921875\n",
            " 56190.0625     50447.91796875 47680.9375    ] --> Prediction: 48783.60546875\n",
            "\n",
            "Predicing on: \n",
            " [45226.55078125 46871.37109375 48359.44921875 56190.0625\n",
            " 50447.91796875 47680.9375     48783.60546875] --> Prediction: 44604.85546875\n",
            "\n",
            "Predicing on: \n",
            " [46871.37109375 48359.44921875 56190.0625     50447.91796875\n",
            " 47680.9375     48783.60546875 44604.85546875] --> Prediction: 48243.453125\n",
            "\n",
            "Predicing on: \n",
            " [48359.44921875 56190.0625     50447.91796875 47680.9375\n",
            " 48783.60546875 44604.85546875 48243.453125  ] --> Prediction: 48603.375\n",
            "\n",
            "Predicing on: \n",
            " [56190.0625     50447.91796875 47680.9375     48783.60546875\n",
            " 44604.85546875 48243.453125   48603.375     ] --> Prediction: 55083.24609375\n",
            "\n",
            "Predicing on: \n",
            " [50447.91796875 47680.9375     48783.60546875 44604.85546875\n",
            " 48243.453125   48603.375      55083.24609375] --> Prediction: 51353.25390625\n",
            "\n",
            "Predicing on: \n",
            " [47680.9375     48783.60546875 44604.85546875 48243.453125\n",
            " 48603.375      55083.24609375 51353.25390625] --> Prediction: 47439.8828125\n",
            "\n",
            "Predicing on: \n",
            " [48783.60546875 44604.85546875 48243.453125   48603.375\n",
            " 55083.24609375 51353.25390625 47439.8828125 ] --> Prediction: 48293.5\n",
            "\n",
            "Predicing on: \n",
            " [44604.85546875 48243.453125   48603.375      55083.24609375\n",
            " 51353.25390625 47439.8828125  48293.5       ] --> Prediction: 44489.32421875\n",
            "\n",
            "Predicing on: \n",
            " [48243.453125   48603.375      55083.24609375 51353.25390625\n",
            " 47439.8828125  48293.5        44489.32421875] --> Prediction: 47991.95703125\n",
            "\n",
            "Predicing on: \n",
            " [48603.375      55083.24609375 51353.25390625 47439.8828125\n",
            " 48293.5        44489.32421875 47991.95703125] --> Prediction: 50296.58203125\n",
            "\n",
            "Predicing on: \n",
            " [55083.24609375 51353.25390625 47439.8828125  48293.5\n",
            " 44489.32421875 47991.95703125 50296.58203125] --> Prediction: 56037.671875\n",
            "\n",
            "Predicing on: \n",
            " [51353.25390625 47439.8828125  48293.5        44489.32421875\n",
            " 47991.95703125 50296.58203125 56037.671875  ] --> Prediction: 50213.27734375\n",
            "\n",
            "Predicing on: \n",
            " [47439.8828125  48293.5        44489.32421875 47991.95703125\n",
            " 50296.58203125 56037.671875   50213.27734375] --> Prediction: 47794.171875\n",
            "\n",
            "Predicing on: \n",
            " [48293.5        44489.32421875 47991.95703125 50296.58203125\n",
            " 56037.671875   50213.27734375 47794.171875  ] --> Prediction: 47922.98046875\n",
            "\n",
            "Predicing on: \n",
            " [44489.32421875 47991.95703125 50296.58203125 56037.671875\n",
            " 50213.27734375 47794.171875   47922.98046875] --> Prediction: 44947.21875\n",
            "\n",
            "Predicing on: \n",
            " [47991.95703125 50296.58203125 56037.671875   50213.27734375\n",
            " 47794.171875   47922.98046875 44947.21875   ] --> Prediction: 48375.6171875\n",
            "\n",
            "Predicing on: \n",
            " [50296.58203125 56037.671875   50213.27734375 47794.171875\n",
            " 47922.98046875 44947.21875    48375.6171875 ] --> Prediction: 51772.734375\n",
            "\n",
            "Predicing on: \n",
            " [56037.671875   50213.27734375 47794.171875   47922.98046875\n",
            " 44947.21875    48375.6171875  51772.734375  ] --> Prediction: 55248.34375\n",
            "\n",
            "Predicing on: \n",
            " [50213.27734375 47794.171875   47922.98046875 44947.21875\n",
            " 48375.6171875  51772.734375   55248.34375   ] --> Prediction: 49048.10546875\n",
            "\n",
            "Predicing on: \n",
            " [47794.171875   47922.98046875 44947.21875    48375.6171875\n",
            " 51772.734375   55248.34375    49048.10546875] --> Prediction: 48109.94140625\n",
            "\n",
            "Predicing on: \n",
            " [47922.98046875 44947.21875    48375.6171875  51772.734375\n",
            " 55248.34375    49048.10546875 48109.94140625] --> Prediction: 47497.953125\n",
            "\n",
            "Predicing on: \n",
            " [44947.21875    48375.6171875  51772.734375   55248.34375\n",
            " 49048.10546875 48109.94140625 47497.953125  ] --> Prediction: 44966.90625\n",
            "\n",
            "Predicing on: \n",
            " [48375.6171875  51772.734375   55248.34375    49048.10546875\n",
            " 48109.94140625 47497.953125   44966.90625   ] --> Prediction: 48770.10546875\n",
            "\n",
            "Predicing on: \n",
            " [51772.734375   55248.34375    49048.10546875 48109.94140625\n",
            " 47497.953125   44966.90625    48770.10546875] --> Prediction: 51409.859375\n",
            "\n",
            "Predicing on: \n",
            " [55248.34375    49048.10546875 48109.94140625 47497.953125\n",
            " 44966.90625    48770.10546875 51409.859375  ] --> Prediction: 54158.75390625\n",
            "\n",
            "Predicing on: \n",
            " [49048.10546875 48109.94140625 47497.953125   44966.90625\n",
            " 48770.10546875 51409.859375   54158.75390625] --> Prediction: 48739.78515625\n",
            "\n",
            "Predicing on: \n",
            " [48109.94140625 47497.953125   44966.90625    48770.10546875\n",
            " 51409.859375   54158.75390625 48739.78515625] --> Prediction: 48610.70703125\n",
            "\n",
            "Predicing on: \n",
            " [47497.953125   44966.90625    48770.10546875 51409.859375\n",
            " 54158.75390625 48739.78515625 48610.70703125] --> Prediction: 47363.625\n",
            "\n",
            "Predicing on: \n",
            " [44966.90625    48770.10546875 51409.859375   54158.75390625\n",
            " 48739.78515625 48610.70703125 47363.625     ] --> Prediction: 45015.22265625\n",
            "\n",
            "Predicing on: \n",
            " [48770.10546875 51409.859375   54158.75390625 48739.78515625\n",
            " 48610.70703125 47363.625      45015.22265625] --> Prediction: 49102.53125\n",
            "\n",
            "Predicing on: \n",
            " [51409.859375   54158.75390625 48739.78515625 48610.70703125\n",
            " 47363.625      45015.22265625 49102.53125   ] --> Prediction: 51929.75\n",
            "\n",
            "Predicing on: \n",
            " [54158.75390625 48739.78515625 48610.70703125 47363.625\n",
            " 45015.22265625 49102.53125    51929.75      ] --> Prediction: 53962.76171875\n",
            "\n",
            "Predicing on: \n",
            " [48739.78515625 48610.70703125 47363.625      45015.22265625\n",
            " 49102.53125    51929.75       53962.76171875] --> Prediction: 48106.60546875\n",
            "\n",
            "Predicing on: \n",
            " [48610.70703125 47363.625      45015.22265625 49102.53125\n",
            " 51929.75       53962.76171875 48106.60546875] --> Prediction: 48915.38671875\n",
            "\n",
            "Predicing on: \n",
            " [47363.625      45015.22265625 49102.53125    51929.75\n",
            " 53962.76171875 48106.60546875 48915.38671875] --> Prediction: 46684.4609375\n",
            "\n",
            "Predicing on: \n",
            " [45015.22265625 49102.53125    51929.75       53962.76171875\n",
            " 48106.60546875 48915.38671875 46684.4609375 ] --> Prediction: 45773.8359375\n",
            "\n",
            "Predicing on: \n",
            " [49102.53125    51929.75       53962.76171875 48106.60546875\n",
            " 48915.38671875 46684.4609375  45773.8359375 ] --> Prediction: 49863.203125\n",
            "\n",
            "Predicing on: \n",
            " [51929.75       53962.76171875 48106.60546875 48915.38671875\n",
            " 46684.4609375  45773.8359375  49863.203125  ] --> Prediction: 52461.375\n",
            "\n",
            "Predicing on: \n",
            " [53962.76171875 48106.60546875 48915.38671875 46684.4609375\n",
            " 45773.8359375  49863.203125   52461.375     ] --> Prediction: 52612.60546875\n",
            "\n",
            "Predicing on: \n",
            " [48106.60546875 48915.38671875 46684.4609375  45773.8359375\n",
            " 49863.203125   52461.375      52612.60546875] --> Prediction: 48027.40625\n",
            "\n",
            "Predicing on: \n",
            " [48915.38671875 46684.4609375  45773.8359375  49863.203125\n",
            " 52461.375      52612.60546875 48027.40625   ] --> Prediction: 48855.7109375\n",
            "\n",
            "Predicing on: \n",
            " [46684.4609375  45773.8359375  49863.203125   52461.375\n",
            " 52612.60546875 48027.40625    48855.7109375 ] --> Prediction: 46378.02734375\n",
            "\n",
            "Predicing on: \n",
            " [45773.8359375  49863.203125   52461.375      52612.60546875\n",
            " 48027.40625    48855.7109375  46378.02734375] --> Prediction: 46437.4609375\n",
            "\n",
            "Predicing on: \n",
            " [49863.203125   52461.375      52612.60546875 48027.40625\n",
            " 48855.7109375  46378.02734375 46437.4609375 ] --> Prediction: 50805.31640625\n",
            "\n",
            "Predicing on: \n",
            " [52461.375      52612.60546875 48027.40625    48855.7109375\n",
            " 46378.02734375 46437.4609375  50805.31640625] --> Prediction: 52369.68359375\n",
            "\n",
            "Predicing on: \n",
            " [52612.60546875 48027.40625    48855.7109375  46378.02734375\n",
            " 46437.4609375  50805.31640625 52369.68359375] --> Prediction: 52112.84765625\n",
            "\n",
            "Predicing on: \n",
            " [48027.40625    48855.7109375  46378.02734375 46437.4609375\n",
            " 50805.31640625 52369.68359375 52112.84765625] --> Prediction: 48653.1875\n",
            "\n",
            "Predicing on: \n",
            " [48855.7109375  46378.02734375 46437.4609375  50805.31640625\n",
            " 52369.68359375 52112.84765625 48653.1875    ] --> Prediction: 48770.8125\n",
            "\n",
            "Predicing on: \n",
            " [46378.02734375 46437.4609375  50805.31640625 52369.68359375\n",
            " 52112.84765625 48653.1875     48770.8125    ] --> Prediction: 45977.984375\n",
            "\n",
            "Predicing on: \n",
            " [46437.4609375  50805.31640625 52369.68359375 52112.84765625\n",
            " 48653.1875     48770.8125     45977.984375  ] --> Prediction: 46981.640625\n",
            "\n",
            "Predicing on: \n",
            " [50805.31640625 52369.68359375 52112.84765625 48653.1875\n",
            " 48770.8125     45977.984375   46981.640625  ] --> Prediction: 50927.07421875\n",
            "\n",
            "Predicing on: \n",
            " [52369.68359375 52112.84765625 48653.1875     48770.8125\n",
            " 45977.984375   46981.640625   50927.07421875] --> Prediction: 52614.93359375\n",
            "\n",
            "Predicing on: \n",
            " [52112.84765625 48653.1875     48770.8125     45977.984375\n",
            " 46981.640625   50927.07421875 52614.93359375] --> Prediction: 51821.03125\n",
            "\n",
            "Predicing on: \n",
            " [48653.1875     48770.8125     45977.984375   46981.640625\n",
            " 50927.07421875 52614.93359375 51821.03125   ] --> Prediction: 48288.78125\n",
            "\n",
            "Predicing on: \n",
            " [48770.8125     45977.984375   46981.640625   50927.07421875\n",
            " 52614.93359375 51821.03125    48288.78125   ] --> Prediction: 48890.05859375\n",
            "\n",
            "Predicing on: \n",
            " [45977.984375   46981.640625   50927.07421875 52614.93359375\n",
            " 51821.03125    48288.78125    48890.05859375] --> Prediction: 45596.31640625\n",
            "\n",
            "Predicing on: \n",
            " [46981.640625   50927.07421875 52614.93359375 51821.03125\n",
            " 48288.78125    48890.05859375 45596.31640625] --> Prediction: 47296.37890625\n",
            "\n",
            "Predicing on: \n",
            " [50927.07421875 52614.93359375 51821.03125    48288.78125\n",
            " 48890.05859375 45596.31640625 47296.37890625] --> Prediction: 51332.67578125\n",
            "\n",
            "Predicing on: \n",
            " [52614.93359375 51821.03125    48288.78125    48890.05859375\n",
            " 45596.31640625 47296.37890625 51332.67578125] --> Prediction: 53093.796875\n",
            "\n",
            "Predicing on: \n",
            " [51821.03125    48288.78125    48890.05859375 45596.31640625\n",
            " 47296.37890625 51332.67578125 53093.796875  ] --> Prediction: 50839.84375\n",
            "\n",
            "Predicing on: \n",
            " [48288.78125    48890.05859375 45596.31640625 47296.37890625\n",
            " 51332.67578125 53093.796875   50839.84375   ] --> Prediction: 48140.5859375\n",
            "\n",
            "Predicing on: \n",
            " [48890.05859375 45596.31640625 47296.37890625 51332.67578125\n",
            " 53093.796875   50839.84375    48140.5859375 ] --> Prediction: 47545.375\n",
            "\n",
            "Predicing on: \n",
            " [45596.31640625 47296.37890625 51332.67578125 53093.796875\n",
            " 50839.84375    48140.5859375  47545.375     ] --> Prediction: 45583.70703125\n",
            "\n",
            "Predicing on: \n",
            " [47296.37890625 51332.67578125 53093.796875   50839.84375\n",
            " 48140.5859375  47545.375      45583.70703125] --> Prediction: 47846.91015625\n",
            "\n",
            "Predicing on: \n",
            " [51332.67578125 53093.796875   50839.84375    48140.5859375\n",
            " 47545.375      45583.70703125 47846.91015625] --> Prediction: 52418.6328125\n",
            "\n",
            "Predicing on: \n",
            " [53093.796875   50839.84375    48140.5859375  47545.375\n",
            " 45583.70703125 47846.91015625 52418.6328125 ] --> Prediction: 52538.51953125\n",
            "\n",
            "Predicing on: \n",
            " [50839.84375    48140.5859375  47545.375      45583.70703125\n",
            " 47846.91015625 52418.6328125  52538.51953125] --> Prediction: 50875.95703125\n",
            "\n",
            "Predicing on: \n",
            " [48140.5859375  47545.375      45583.70703125 47846.91015625\n",
            " 52418.6328125  52538.51953125 50875.95703125] --> Prediction: 48075.88671875\n",
            "\n",
            "Predicing on: \n",
            " [47545.375      45583.70703125 47846.91015625 52418.6328125\n",
            " 52538.51953125 50875.95703125 48075.88671875] --> Prediction: 46688.34375\n",
            "\n",
            "Predicing on: \n",
            " [45583.70703125 47846.91015625 52418.6328125  52538.51953125\n",
            " 50875.95703125 48075.88671875 46688.34375   ] --> Prediction: 44905.19921875\n",
            "\n",
            "Predicing on: \n",
            " [47846.91015625 52418.6328125  52538.51953125 50875.95703125\n",
            " 48075.88671875 46688.34375    44905.19921875] --> Prediction: 48358.83203125\n",
            "\n",
            "Predicing on: \n",
            " [52418.6328125  52538.51953125 50875.95703125 48075.88671875\n",
            " 46688.34375    44905.19921875 48358.83203125] --> Prediction: 52598.07421875\n",
            "\n",
            "Predicing on: \n",
            " [52538.51953125 50875.95703125 48075.88671875 46688.34375\n",
            " 44905.19921875 48358.83203125 52598.07421875] --> Prediction: 52164.5234375\n",
            "\n",
            "Predicing on: \n",
            " [50875.95703125 48075.88671875 46688.34375    44905.19921875\n",
            " 48358.83203125 52598.07421875 52164.5234375 ] --> Prediction: 49886.11328125\n",
            "\n",
            "Predicing on: \n",
            " [48075.88671875 46688.34375    44905.19921875 48358.83203125\n",
            " 52598.07421875 52164.5234375  49886.11328125] --> Prediction: 47778.4296875\n",
            "\n",
            "Predicing on: \n",
            " [46688.34375    44905.19921875 48358.83203125 52598.07421875\n",
            " 52164.5234375  49886.11328125 47778.4296875 ] --> Prediction: 46047.91015625\n",
            "\n",
            "Predicing on: \n",
            " [44905.19921875 48358.83203125 52598.07421875 52164.5234375\n",
            " 49886.11328125 47778.4296875  46047.91015625] --> Prediction: 45019.09375\n",
            "\n",
            "Predicing on: \n",
            " [48358.83203125 52598.07421875 52164.5234375  49886.11328125\n",
            " 47778.4296875  46047.91015625 45019.09375   ] --> Prediction: 48598.078125\n",
            "\n",
            "Predicing on: \n",
            " [52598.07421875 52164.5234375  49886.11328125 47778.4296875\n",
            " 46047.91015625 45019.09375    48598.078125  ] --> Prediction: 53040.87890625\n",
            "\n",
            "Predicing on: \n",
            " [52164.5234375  49886.11328125 47778.4296875  46047.91015625\n",
            " 45019.09375    48598.078125   53040.87890625] --> Prediction: 53271.484375\n",
            "\n",
            "Predicing on: \n",
            " [49886.11328125 47778.4296875  46047.91015625 45019.09375\n",
            " 48598.078125   53040.87890625 53271.484375  ] --> Prediction: 49587.24609375\n",
            "\n",
            "Predicing on: \n",
            " [47778.4296875  46047.91015625 45019.09375    48598.078125\n",
            " 53040.87890625 53271.484375   49587.24609375] --> Prediction: 47451.86328125\n",
            "\n",
            "Predicing on: \n",
            " [46047.91015625 45019.09375    48598.078125   53040.87890625\n",
            " 53271.484375   49587.24609375 47451.86328125] --> Prediction: 45860.8671875\n",
            "\n",
            "Predicing on: \n",
            " [45019.09375    48598.078125   53040.87890625 53271.484375\n",
            " 49587.24609375 47451.86328125 45860.8671875 ] --> Prediction: 44894.11328125\n",
            "\n",
            "Predicing on: \n",
            " [48598.078125   53040.87890625 53271.484375   49587.24609375\n",
            " 47451.86328125 45860.8671875  44894.11328125] --> Prediction: 49513.453125\n",
            "\n",
            "Predicing on: \n",
            " [53040.87890625 53271.484375   49587.24609375 47451.86328125\n",
            " 45860.8671875  44894.11328125 49513.453125  ] --> Prediction: 54710.046875\n",
            "\n",
            "Predicing on: \n",
            " [53271.484375   49587.24609375 47451.86328125 45860.8671875\n",
            " 44894.11328125 49513.453125   54710.046875  ] --> Prediction: 52219.17578125\n",
            "\n",
            "Predicing on: \n",
            " [49587.24609375 47451.86328125 45860.8671875  44894.11328125\n",
            " 49513.453125   54710.046875   52219.17578125] --> Prediction: 49869.328125\n",
            "\n",
            "Predicing on: \n",
            " [47451.86328125 45860.8671875  44894.11328125 49513.453125\n",
            " 54710.046875   52219.17578125 49869.328125  ] --> Prediction: 46663.4140625\n",
            "\n",
            "Predicing on: \n",
            " [45860.8671875  44894.11328125 49513.453125   54710.046875\n",
            " 52219.17578125 49869.328125   46663.4140625 ] --> Prediction: 45998.78515625\n",
            "\n",
            "Predicing on: \n",
            " [44894.11328125 49513.453125   54710.046875   52219.17578125\n",
            " 49869.328125   46663.4140625  45998.78515625] --> Prediction: 45357.3359375\n",
            "\n",
            "Predicing on: \n",
            " [49513.453125   54710.046875   52219.17578125 49869.328125\n",
            " 46663.4140625  45998.78515625 45357.3359375 ] --> Prediction: 50705.09765625\n",
            "\n",
            "Predicing on: \n",
            " [54710.046875   52219.17578125 49869.328125   46663.4140625\n",
            " 45998.78515625 45357.3359375  50705.09765625] --> Prediction: 54287.79296875\n",
            "\n",
            "Predicing on: \n",
            " [52219.17578125 49869.328125   46663.4140625  45998.78515625\n",
            " 45357.3359375  50705.09765625 54287.79296875] --> Prediction: 50848.41796875\n",
            "\n",
            "Predicing on: \n",
            " [49869.328125   46663.4140625  45998.78515625 45357.3359375\n",
            " 50705.09765625 54287.79296875 50848.41796875] --> Prediction: 50275.640625\n",
            "\n",
            "Predicing on: \n",
            " [46663.4140625  45998.78515625 45357.3359375  50705.09765625\n",
            " 54287.79296875 50848.41796875 50275.640625  ] --> Prediction: 46854.4609375\n",
            "\n",
            "Predicing on: \n",
            " [45998.78515625 45357.3359375  50705.09765625 54287.79296875\n",
            " 50848.41796875 50275.640625   46854.4609375 ] --> Prediction: 45343.92578125\n",
            "\n",
            "Predicing on: \n",
            " [45357.3359375  50705.09765625 54287.79296875 50848.41796875\n",
            " 50275.640625   46854.4609375  45343.92578125] --> Prediction: 45761.57421875\n",
            "\n",
            "Predicing on: \n",
            " [50705.09765625 54287.79296875 50848.41796875 50275.640625\n",
            " 46854.4609375  45343.92578125 45761.57421875] --> Prediction: 50412.9765625\n",
            "\n",
            "Predicing on: \n",
            " [54287.79296875 50848.41796875 50275.640625   46854.4609375\n",
            " 45343.92578125 45761.57421875 50412.9765625 ] --> Prediction: 53810.5546875\n",
            "\n",
            "Predicing on: \n",
            " [50848.41796875 50275.640625   46854.4609375  45343.92578125\n",
            " 45761.57421875 50412.9765625  53810.5546875 ] --> Prediction: 51341.41015625\n",
            "\n",
            "Predicing on: \n",
            " [50275.640625   46854.4609375  45343.92578125 45761.57421875\n",
            " 50412.9765625  53810.5546875  51341.41015625] --> Prediction: 49949.07421875\n",
            "\n",
            "Predicing on: \n",
            " [46854.4609375  45343.92578125 45761.57421875 50412.9765625\n",
            " 53810.5546875  51341.41015625 49949.07421875] --> Prediction: 46650.6640625\n",
            "\n",
            "Predicing on: \n",
            " [45343.92578125 45761.57421875 50412.9765625  53810.5546875\n",
            " 51341.41015625 49949.07421875 46650.6640625 ] --> Prediction: 44744.19140625\n",
            "\n",
            "Predicing on: \n",
            " [45761.57421875 50412.9765625  53810.5546875  51341.41015625\n",
            " 49949.07421875 46650.6640625  44744.19140625] --> Prediction: 45634.83203125\n",
            "\n",
            "Predicing on: \n",
            " [50412.9765625  53810.5546875  51341.41015625 49949.07421875\n",
            " 46650.6640625  44744.19140625 45634.83203125] --> Prediction: 51555.95703125\n",
            "\n",
            "Predicing on: \n",
            " [53810.5546875  51341.41015625 49949.07421875 46650.6640625\n",
            " 44744.19140625 45634.83203125 51555.95703125] --> Prediction: 54462.875\n",
            "\n",
            "Predicing on: \n",
            " [51341.41015625 49949.07421875 46650.6640625  44744.19140625\n",
            " 45634.83203125 51555.95703125 54462.875     ] --> Prediction: 50398.2890625\n",
            "\n",
            "Predicing on: \n",
            " [49949.07421875 46650.6640625  44744.19140625 45634.83203125\n",
            " 51555.95703125 54462.875      50398.2890625 ] --> Prediction: 49774.13671875\n",
            "\n",
            "Predicing on: \n",
            " [46650.6640625  44744.19140625 45634.83203125 51555.95703125\n",
            " 54462.875      50398.2890625  49774.13671875] --> Prediction: 45945.9296875\n",
            "\n",
            "Predicing on: \n",
            " [44744.19140625 45634.83203125 51555.95703125 54462.875\n",
            " 50398.2890625  49774.13671875 45945.9296875 ] --> Prediction: 46034.65234375\n",
            "\n",
            "Predicing on: \n",
            " [45634.83203125 51555.95703125 54462.875      50398.2890625\n",
            " 49774.13671875 45945.9296875  46034.65234375] --> Prediction: 46581.7578125\n",
            "\n",
            "Predicing on: \n",
            " [51555.95703125 54462.875      50398.2890625  49774.13671875\n",
            " 45945.9296875  46034.65234375 46581.7578125 ] --> Prediction: 51910.25390625\n",
            "\n",
            "Predicing on: \n",
            " [54462.875      50398.2890625  49774.13671875 45945.9296875\n",
            " 46034.65234375 46581.7578125  51910.25390625] --> Prediction: 53229.70703125\n",
            "\n",
            "Predicing on: \n",
            " [50398.2890625  49774.13671875 45945.9296875  46034.65234375\n",
            " 46581.7578125  51910.25390625 53229.70703125] --> Prediction: 49622.22265625\n",
            "\n",
            "Predicing on: \n",
            " [49774.13671875 45945.9296875  46034.65234375 46581.7578125\n",
            " 51910.25390625 53229.70703125 49622.22265625] --> Prediction: 49032.2109375\n",
            "\n",
            "Predicing on: \n",
            " [45945.9296875  46034.65234375 46581.7578125  51910.25390625\n",
            " 53229.70703125 49622.22265625 49032.2109375 ] --> Prediction: 45886.3828125\n",
            "\n",
            "Predicing on: \n",
            " [46034.65234375 46581.7578125  51910.25390625 53229.70703125\n",
            " 49622.22265625 49032.2109375  45886.3828125 ] --> Prediction: 45957.78125\n",
            "\n",
            "Predicing on: \n",
            " [46581.7578125  51910.25390625 53229.70703125 49622.22265625\n",
            " 49032.2109375  45886.3828125  45957.78125   ] --> Prediction: 47753.51171875\n",
            "\n",
            "Predicing on: \n",
            " [51910.25390625 53229.70703125 49622.22265625 49032.2109375\n",
            " 45886.3828125  45957.78125    47753.51171875] --> Prediction: 52069.53125\n",
            "\n",
            "Predicing on: \n",
            " [53229.70703125 49622.22265625 49032.2109375  45886.3828125\n",
            " 45957.78125    47753.51171875 52069.53125   ] --> Prediction: 52549.1796875\n",
            "\n",
            "Predicing on: \n",
            " [49622.22265625 49032.2109375  45886.3828125  45957.78125\n",
            " 47753.51171875 52069.53125    52549.1796875 ] --> Prediction: 50046.5859375\n",
            "\n",
            "Predicing on: \n",
            " [49032.2109375  45886.3828125  45957.78125    47753.51171875\n",
            " 52069.53125    52549.1796875  50046.5859375 ] --> Prediction: 48600.859375\n",
            "\n",
            "Predicing on: \n",
            " [45886.3828125  45957.78125    47753.51171875 52069.53125\n",
            " 52549.1796875  50046.5859375  48600.859375  ] --> Prediction: 45452.82421875\n",
            "\n",
            "Predicing on: \n",
            " [45957.78125    47753.51171875 52069.53125    52549.1796875\n",
            " 50046.5859375  48600.859375   45452.82421875] --> Prediction: 46004.0390625\n",
            "\n",
            "Predicing on: \n",
            " [47753.51171875 52069.53125    52549.1796875  50046.5859375\n",
            " 48600.859375   45452.82421875 46004.0390625 ] --> Prediction: 47991.48828125\n",
            "\n",
            "Predicing on: \n",
            " [52069.53125    52549.1796875  50046.5859375  48600.859375\n",
            " 45452.82421875 46004.0390625  47991.48828125] --> Prediction: 52539.3046875\n",
            "\n",
            "Predicing on: \n",
            " [52549.1796875  50046.5859375  48600.859375   45452.82421875\n",
            " 46004.0390625  47991.48828125 52539.3046875 ] --> Prediction: 52751.546875\n",
            "\n",
            "Predicing on: \n",
            " [50046.5859375  48600.859375   45452.82421875 46004.0390625\n",
            " 47991.48828125 52539.3046875  52751.546875  ] --> Prediction: 49290.29296875\n",
            "\n",
            "Predicing on: \n",
            " [48600.859375   45452.82421875 46004.0390625  47991.48828125\n",
            " 52539.3046875  52751.546875   49290.29296875] --> Prediction: 48572.58203125\n",
            "\n",
            "Predicing on: \n",
            " [45452.82421875 46004.0390625  47991.48828125 52539.3046875\n",
            " 52751.546875   49290.29296875 48572.58203125] --> Prediction: 45045.06640625\n",
            "\n",
            "Predicing on: \n",
            " [46004.0390625  47991.48828125 52539.3046875  52751.546875\n",
            " 49290.29296875 48572.58203125 45045.06640625] --> Prediction: 46138.46875\n",
            "\n",
            "Predicing on: \n",
            " [47991.48828125 52539.3046875  52751.546875   49290.29296875\n",
            " 48572.58203125 45045.06640625 46138.46875   ] --> Prediction: 49176.55078125\n",
            "\n",
            "Predicing on: \n",
            " [52539.3046875  52751.546875   49290.29296875 48572.58203125\n",
            " 45045.06640625 46138.46875    49176.55078125] --> Prediction: 53275.3203125\n",
            "\n",
            "Predicing on: \n",
            " [52751.546875   49290.29296875 48572.58203125 45045.06640625\n",
            " 46138.46875    49176.55078125 53275.3203125 ] --> Prediction: 51672.90234375\n",
            "\n",
            "Predicing on: \n",
            " [49290.29296875 48572.58203125 45045.06640625 46138.46875\n",
            " 49176.55078125 53275.3203125  51672.90234375] --> Prediction: 48959.81640625\n",
            "\n",
            "Predicing on: \n",
            " [48572.58203125 45045.06640625 46138.46875    49176.55078125\n",
            " 53275.3203125  51672.90234375 48959.81640625] --> Prediction: 47471.046875\n",
            "\n",
            "Predicing on: \n",
            " [45045.06640625 46138.46875    49176.55078125 53275.3203125\n",
            " 51672.90234375 48959.81640625 47471.046875  ] --> Prediction: 45293.56640625\n",
            "\n",
            "Predicing on: \n",
            " [46138.46875    49176.55078125 53275.3203125  51672.90234375\n",
            " 48959.81640625 47471.046875   45293.56640625] --> Prediction: 46468.421875\n",
            "\n",
            "Predicing on: \n",
            " [49176.55078125 53275.3203125  51672.90234375 48959.81640625\n",
            " 47471.046875   45293.56640625 46468.421875  ] --> Prediction: 50103.7265625\n",
            "\n",
            "Predicing on: \n",
            " [53275.3203125  51672.90234375 48959.81640625 47471.046875\n",
            " 45293.56640625 46468.421875   50103.7265625 ] --> Prediction: 52965.39453125\n",
            "\n",
            "Predicing on: \n",
            " [51672.90234375 48959.81640625 47471.046875   45293.56640625\n",
            " 46468.421875   50103.7265625  52965.39453125] --> Prediction: 51413.859375\n",
            "\n",
            "Predicing on: \n",
            " [48959.81640625 47471.046875   45293.56640625 46468.421875\n",
            " 50103.7265625  52965.39453125 51413.859375  ] --> Prediction: 49252.9765625\n",
            "\n",
            "Predicing on: \n",
            " [47471.046875   45293.56640625 46468.421875   50103.7265625\n",
            " 52965.39453125 51413.859375   49252.9765625 ] --> Prediction: 46713.5703125\n",
            "\n",
            "Predicing on: \n",
            " [45293.56640625 46468.421875   50103.7265625  52965.39453125\n",
            " 51413.859375   49252.9765625  46713.5703125 ] --> Prediction: 44814.25390625\n",
            "\n",
            "Predicing on: \n",
            " [46468.421875   50103.7265625  52965.39453125 51413.859375\n",
            " 49252.9765625  46713.5703125  44814.25390625] --> Prediction: 46840.125\n",
            "\n",
            "Predicing on: \n",
            " [50103.7265625  52965.39453125 51413.859375   49252.9765625\n",
            " 46713.5703125  44814.25390625 46840.125     ] --> Prediction: 50364.140625\n",
            "\n",
            "Predicing on: \n",
            " [52965.39453125 51413.859375   49252.9765625  46713.5703125\n",
            " 44814.25390625 46840.125      50364.140625  ] --> Prediction: 53060.98046875\n",
            "\n",
            "Predicing on: \n",
            " [51413.859375   49252.9765625  46713.5703125  44814.25390625\n",
            " 46840.125      50364.140625   53060.98046875] --> Prediction: 50842.60546875\n",
            "\n",
            "Predicing on: \n",
            " [49252.9765625  46713.5703125  44814.25390625 46840.125\n",
            " 50364.140625   53060.98046875 50842.60546875] --> Prediction: 48677.69140625\n",
            "\n",
            "Predicing on: \n",
            " [46713.5703125  44814.25390625 46840.125      50364.140625\n",
            " 53060.98046875 50842.60546875 48677.69140625] --> Prediction: 46310.91015625\n",
            "\n",
            "Predicing on: \n",
            " [44814.25390625 46840.125      50364.140625   53060.98046875\n",
            " 50842.60546875 48677.69140625 46310.91015625] --> Prediction: 44374.0859375\n",
            "\n",
            "Predicing on: \n",
            " [46840.125      50364.140625   53060.98046875 50842.60546875\n",
            " 48677.69140625 46310.91015625 44374.0859375 ] --> Prediction: 46922.0390625\n",
            "\n",
            "Predicing on: \n",
            " [50364.140625   53060.98046875 50842.60546875 48677.69140625\n",
            " 46310.91015625 44374.0859375  46922.0390625 ] --> Prediction: 51101.1796875\n",
            "\n",
            "Predicing on: \n",
            " [53060.98046875 50842.60546875 48677.69140625 46310.91015625\n",
            " 44374.0859375  46922.0390625  51101.1796875 ] --> Prediction: 54167.953125\n",
            "\n",
            "Predicing on: \n",
            " [50842.60546875 48677.69140625 46310.91015625 44374.0859375\n",
            " 46922.0390625  51101.1796875  54167.953125  ] --> Prediction: 50425.40234375\n",
            "\n",
            "Predicing on: \n",
            " [48677.69140625 46310.91015625 44374.0859375  46922.0390625\n",
            " 51101.1796875  54167.953125   50425.40234375] --> Prediction: 48412.27734375\n",
            "\n",
            "Predicing on: \n",
            " [46310.91015625 44374.0859375  46922.0390625  51101.1796875\n",
            " 54167.953125   50425.40234375 48412.27734375] --> Prediction: 46066.640625\n",
            "\n",
            "Predicing on: \n",
            " [44374.0859375  46922.0390625  51101.1796875  54167.953125\n",
            " 50425.40234375 48412.27734375 46066.640625  ] --> Prediction: 44207.23828125\n",
            "\n",
            "Predicing on: \n",
            " [46922.0390625  51101.1796875  54167.953125   50425.40234375\n",
            " 48412.27734375 46066.640625   44207.23828125] --> Prediction: 47459.0078125\n",
            "\n",
            "Predicing on: \n",
            " [51101.1796875  54167.953125   50425.40234375 48412.27734375\n",
            " 46066.640625   44207.23828125 47459.0078125 ] --> Prediction: 52372.8984375\n",
            "\n",
            "Predicing on: \n",
            " [54167.953125   50425.40234375 48412.27734375 46066.640625\n",
            " 44207.23828125 47459.0078125  52372.8984375 ] --> Prediction: 53553.30078125\n",
            "\n",
            "Predicing on: \n",
            " [50425.40234375 48412.27734375 46066.640625   44207.23828125\n",
            " 47459.0078125  52372.8984375  53553.30078125] --> Prediction: 49982.6171875\n",
            "\n",
            "Predicing on: \n",
            " [48412.27734375 46066.640625   44207.23828125 47459.0078125\n",
            " 52372.8984375  53553.30078125 49982.6171875 ] --> Prediction: 47234.33203125\n",
            "\n",
            "Predicing on: \n",
            " [46066.640625   44207.23828125 47459.0078125  52372.8984375\n",
            " 53553.30078125 49982.6171875  47234.33203125] --> Prediction: 45810.3671875\n",
            "\n",
            "Predicing on: \n",
            " [44207.23828125 47459.0078125  52372.8984375  53553.30078125\n",
            " 49982.6171875  47234.33203125 45810.3671875 ] --> Prediction: 44147.0625\n",
            "\n",
            "Predicing on: \n",
            " [47459.0078125  52372.8984375  53553.30078125 49982.6171875\n",
            " 47234.33203125 45810.3671875  44147.0625    ] --> Prediction: 48836.3515625\n",
            "\n",
            "Predicing on: \n",
            " [52372.8984375  53553.30078125 49982.6171875  47234.33203125\n",
            " 45810.3671875  44147.0625     48836.3515625 ] --> Prediction: 52220.5546875\n",
            "\n",
            "Predicing on: \n",
            " [53553.30078125 49982.6171875  47234.33203125 45810.3671875\n",
            " 44147.0625     48836.3515625  52220.5546875 ] --> Prediction: 52044.8359375\n",
            "\n",
            "Predicing on: \n",
            " [49982.6171875  47234.33203125 45810.3671875  44147.0625\n",
            " 48836.3515625  52220.5546875  52044.8359375 ] --> Prediction: 50463.33203125\n",
            "\n",
            "Predicing on: \n",
            " [47234.33203125 45810.3671875  44147.0625     48836.3515625\n",
            " 52220.5546875  52044.8359375  50463.33203125] --> Prediction: 47191.58203125\n",
            "\n",
            "Predicing on: \n",
            " [45810.3671875  44147.0625     48836.3515625  52220.5546875\n",
            " 52044.8359375  50463.33203125 47191.58203125] --> Prediction: 44972.109375\n",
            "\n",
            "Predicing on: \n",
            " [44147.0625     48836.3515625  52220.5546875  52044.8359375\n",
            " 50463.33203125 47191.58203125 44972.109375  ] --> Prediction: 44160.98046875\n",
            "\n",
            "Predicing on: \n",
            " [48836.3515625  52220.5546875  52044.8359375  50463.33203125\n",
            " 47191.58203125 44972.109375   44160.98046875] --> Prediction: 48415.4609375\n",
            "\n",
            "Predicing on: \n",
            " [52220.5546875  52044.8359375  50463.33203125 47191.58203125\n",
            " 44972.109375   44160.98046875 48415.4609375 ] --> Prediction: 52522.421875\n",
            "\n",
            "Predicing on: \n",
            " [52044.8359375  50463.33203125 47191.58203125 44972.109375\n",
            " 44160.98046875 48415.4609375  52522.421875  ] --> Prediction: 52675.04296875\n",
            "\n",
            "Predicing on: \n",
            " [50463.33203125 47191.58203125 44972.109375   44160.98046875\n",
            " 48415.4609375  52522.421875   52675.04296875] --> Prediction: 49644.80078125\n",
            "\n",
            "Predicing on: \n",
            " [47191.58203125 44972.109375   44160.98046875 48415.4609375\n",
            " 52522.421875   52675.04296875 49644.80078125] --> Prediction: 47168.38671875\n",
            "\n",
            "Predicing on: \n",
            " [44972.109375   44160.98046875 48415.4609375  52522.421875\n",
            " 52675.04296875 49644.80078125 47168.38671875] --> Prediction: 44429.17578125\n",
            "\n",
            "Predicing on: \n",
            " [44160.98046875 48415.4609375  52522.421875   52675.04296875\n",
            " 49644.80078125 47168.38671875 44429.17578125] --> Prediction: 43987.64453125\n",
            "\n",
            "Predicing on: \n",
            " [48415.4609375  52522.421875   52675.04296875 49644.80078125\n",
            " 47168.38671875 44429.17578125 43987.64453125] --> Prediction: 49460.15625\n",
            "\n",
            "Predicing on: \n",
            " [52522.421875   52675.04296875 49644.80078125 47168.38671875\n",
            " 44429.17578125 43987.64453125 49460.15625   ] --> Prediction: 53484.859375\n",
            "\n",
            "Predicing on: \n",
            " [52675.04296875 49644.80078125 47168.38671875 44429.17578125\n",
            " 43987.64453125 49460.15625    53484.859375  ] --> Prediction: 51576.0546875\n",
            "\n",
            "Predicing on: \n",
            " [49644.80078125 47168.38671875 44429.17578125 43987.64453125\n",
            " 49460.15625    53484.859375   51576.0546875 ] --> Prediction: 49175.11328125\n",
            "\n",
            "Predicing on: \n",
            " [47168.38671875 44429.17578125 43987.64453125 49460.15625\n",
            " 53484.859375   51576.0546875  49175.11328125] --> Prediction: 46909.7109375\n",
            "\n",
            "Predicing on: \n",
            " [44429.17578125 43987.64453125 49460.15625    53484.859375\n",
            " 51576.0546875  49175.11328125 46909.7109375 ] --> Prediction: 44770.1875\n",
            "\n",
            "Predicing on: \n",
            " [43987.64453125 49460.15625    53484.859375   51576.0546875\n",
            " 49175.11328125 46909.7109375  44770.1875    ] --> Prediction: 44574.625\n",
            "\n",
            "Predicing on: \n",
            " [49460.15625    53484.859375   51576.0546875  49175.11328125\n",
            " 46909.7109375  44770.1875     44574.625     ] --> Prediction: 50088.60546875\n",
            "\n",
            "Predicing on: \n",
            " [53484.859375   51576.0546875  49175.11328125 46909.7109375\n",
            " 44770.1875     44574.625      50088.60546875] --> Prediction: 52335.01953125\n",
            "\n",
            "Predicing on: \n",
            " [51576.0546875  49175.11328125 46909.7109375  44770.1875\n",
            " 44574.625      50088.60546875 52335.01953125] --> Prediction: 50411.58984375\n",
            "\n",
            "Predicing on: \n",
            " [49175.11328125 46909.7109375  44770.1875     44574.625\n",
            " 50088.60546875 52335.01953125 50411.58984375] --> Prediction: 48529.1875\n",
            "\n",
            "Predicing on: \n",
            " [46909.7109375  44770.1875     44574.625      50088.60546875\n",
            " 52335.01953125 50411.58984375 48529.1875    ] --> Prediction: 46983.78125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZvOcaWdyPg3"
      },
      "source": [
        "### 7. For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasnâ€™t retrained for every forecast (model_9)?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD2tfR7Y-BWG"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkfktUGa-CRw"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYvX6FuH-Cci"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzHOR23b-CkE"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uedqTuLr-Cs1"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Udvq9MA-C3S"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eo6JOrd-Dme"
      },
      "source": [
        "### 8. Throughout this notebook, weâ€™ve only tried algorithms weâ€™ve handcrafted ourselves. But itâ€™s worth seeing how a purpose built forecasting algorithm goes.\n",
        "\n",
        "* Try out one of the extra algorithms listed in the modelling experiments part such as:\n",
        "\t*  [Facebookâ€™s Kats library](https://github.com/facebookresearch/Kats)  - there are many models in here, remember the machine learning practionerâ€™s motto: experiment, experiment, experiment.\n",
        "\t*  [LinkedInâ€™s Greykite library](https://github.com/linkedin/greykite) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rngm_dIP-Exz",
        "outputId": "c3bf2bea-ea5b-46d9-add8-9cd410f24f36"
      },
      "source": [
        "# Installing facebooks kats lib \n",
        "!pip install kats "
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kats in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: statsmodels>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from kats) (0.12.2)\n",
            "Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from kats) (0.10.5.2)\n",
            "Requirement already satisfied: pymannkendall>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from kats) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn>+0.24.2 in /usr/local/lib/python3.7/dist-packages (from kats) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from kats) (4.62.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from kats) (2.8.2)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.7/dist-packages (from kats) (1.2)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.7/dist-packages (from kats) (0.0.9)\n",
            "Requirement already satisfied: attrs>=21.2.0 in /usr/local/lib/python3.7/dist-packages (from kats) (21.2.0)\n",
            "Requirement already satisfied: gpytorch in /usr/local/lib/python3.7/dist-packages (from kats) (1.5.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kats) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from kats) (1.19.5)\n",
            "Requirement already satisfied: fbprophet==0.7 in /usr/local/lib/python3.7/dist-packages (from kats) (0.7)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from kats) (2.3.2)\n",
            "Requirement already satisfied: numba>=0.52.0 in /usr/local/lib/python3.7/dist-packages (from kats) (0.54.0)\n",
            "Requirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from kats) (0.11.1)\n",
            "Requirement already satisfied: ax-platform in /usr/local/lib/python3.7/dist-packages (from kats) (0.2.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from kats) (3.2.2)\n",
            "Requirement already satisfied: pystan==2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from kats) (2.19.1.1)\n",
            "Requirement already satisfied: plotly>=4.14.3 in /usr/local/lib/python3.7/dist-packages (from kats) (5.3.1)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from kats) (1.1.5)\n",
            "Requirement already satisfied: cmdstanpy==0.9.5 in /usr/local/lib/python3.7/dist-packages (from fbprophet==0.7->kats) (0.9.5)\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.7/dist-packages (from fbprophet==0.7->kats) (0.29.24)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->kats) (0.5.11)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->kats) (2018.9)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->kats) (2.1.3)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->kats) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->kats) (1.15.0)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->kats) (4.0.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->kats) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->kats) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->kats) (1.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.52.0->kats) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /usr/local/lib/python3.7/dist-packages (from numba>=0.52.0->kats) (0.37.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.14.3->kats) (8.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pymannkendall>=1.4.1->kats) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>+0.24.2->kats) (1.0.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.2->kats) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kats) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from ax-platform->kats) (2.11.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from ax-platform->kats) (2.7.1)\n",
            "Requirement already satisfied: botorch==0.5.0 in /usr/local/lib/python3.7/dist-packages (from ax-platform->kats) (0.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->ax-platform->kats) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R2yY9-oqYd-"
      },
      "source": [
        "# Importing the TimeSerisData class from Kats \n",
        "from kats.consts import TimeSeriesData\n"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFyRBrpIrKns"
      },
      "source": [
        "# Utils to work with Kats \n",
        "from dateutil import parser \n",
        "from datetime import datetime"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BbGVjIvrSCT",
        "outputId": "ecbc827a-c7b7-4dc2-acfc-ebdddab4fd3b"
      },
      "source": [
        "# Creating a Dataset object with Kats \n",
        "ts_data = TimeSeriesData(time = bitcoin_prices_updated.index , \n",
        "               value = bitcoin_prices_updated.Price)\n",
        "\n",
        "type(ts_data)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kats.consts.TimeSeriesData"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "lC4aG5BZraEa",
        "outputId": "90c6f3f8-d12e-4d46-eb46-2bc5f171b08e"
      },
      "source": [
        "# Plotting the timeseries data \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ts_data.plot(cols=['Price'])\n",
        "plt.show()\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB57AkOir5rY"
      },
      "source": [
        "Lets make some forecasting with our data on a prophet model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuNutrLdSUCI",
        "outputId": "e4f188c5-5c93-4710-e0cb-392a5c91141f"
      },
      "source": [
        "# Importing the prophet \n",
        "from kats.models.prophet import ProphetModel , ProphetParams\n",
        "\n",
        "# Creating a model param instance \n",
        "params = ProphetParams(seasonality_mode= 'multiplicative') \n",
        "\n",
        "# Create a prophet model instance (just like how we create an instance in sklearn)\n",
        "model = ProphetModel(ts_data , params)\n",
        "\n",
        "# Fitting the model \n",
        "model.fit() \n",
        "\n",
        "# Making predictions \n",
        "forecast = model.predict(steps= 1 , include_history= True , freq = '1W')"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "jrVnWaygShim",
        "outputId": "5af05705-58f7-4c31-a17f-3898f1755a5c"
      },
      "source": [
        "# Predicting the bitcoin price for a day (Horizon = 1 )\n",
        "forecast.head(10)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>fcst</th>\n",
              "      <th>fcst_lower</th>\n",
              "      <th>fcst_upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-11-02</td>\n",
              "      <td>100.477979</td>\n",
              "      <td>-1948.078014</td>\n",
              "      <td>2020.033974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-11-03</td>\n",
              "      <td>100.366334</td>\n",
              "      <td>-2050.732440</td>\n",
              "      <td>2014.143528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014-11-04</td>\n",
              "      <td>100.533908</td>\n",
              "      <td>-1900.069011</td>\n",
              "      <td>2210.730273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014-11-05</td>\n",
              "      <td>100.078650</td>\n",
              "      <td>-1948.428324</td>\n",
              "      <td>2137.642557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014-11-06</td>\n",
              "      <td>101.371062</td>\n",
              "      <td>-1937.311207</td>\n",
              "      <td>2128.023726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2014-11-07</td>\n",
              "      <td>100.766892</td>\n",
              "      <td>-1782.054259</td>\n",
              "      <td>2138.806242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2014-11-08</td>\n",
              "      <td>101.996420</td>\n",
              "      <td>-1980.782147</td>\n",
              "      <td>2296.454414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2014-11-09</td>\n",
              "      <td>102.773991</td>\n",
              "      <td>-1879.018874</td>\n",
              "      <td>2230.720342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2014-11-10</td>\n",
              "      <td>103.027023</td>\n",
              "      <td>-1799.212598</td>\n",
              "      <td>1962.218218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2014-11-11</td>\n",
              "      <td>103.665574</td>\n",
              "      <td>-1978.564787</td>\n",
              "      <td>2033.804404</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        time        fcst   fcst_lower   fcst_upper\n",
              "0 2014-11-02  100.477979 -1948.078014  2020.033974\n",
              "1 2014-11-03  100.366334 -2050.732440  2014.143528\n",
              "2 2014-11-04  100.533908 -1900.069011  2210.730273\n",
              "3 2014-11-05  100.078650 -1948.428324  2137.642557\n",
              "4 2014-11-06  101.371062 -1937.311207  2128.023726\n",
              "5 2014-11-07  100.766892 -1782.054259  2138.806242\n",
              "6 2014-11-08  101.996420 -1980.782147  2296.454414\n",
              "7 2014-11-09  102.773991 -1879.018874  2230.720342\n",
              "8 2014-11-10  103.027023 -1799.212598  1962.218218\n",
              "9 2014-11-11  103.665574 -1978.564787  2033.804404"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjry8BspjFrj"
      },
      "source": [
        "The above shows the simple implementation of the Kat's library. And we have used a Prophet model with multiplicative seasonlity. \n",
        "\n",
        "Feel free to play with other parameters as well. \n",
        "\n",
        "Next we will look into creating a ensemble of models using the Kat's library. We can achieve this by using `KatEnsemble`, it will be kinda similar to how scikit-learn works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tP71FtbZZS3"
      },
      "source": [
        "# Importing the things we need \n",
        "from kats.models.ensemble.ensemble import EnsembleParams , BaseModelParams \n",
        "from kats.models.ensemble.kats_ensemble import KatsEnsemble\n",
        "from kats.models import (\n",
        "    arima, \n",
        "    holtwinters , \n",
        "    linear_model , \n",
        "    prophet , \n",
        "    quadratic_model , \n",
        "    sarima , \n",
        "    theta\n",
        ")"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL_oYp-7ZxUo"
      },
      "source": [
        "On the next step we will define parameters for each individual forecasting model using the `EnsembleParams` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZLW0KdWZ4ji"
      },
      "source": [
        "# Defining the parameters of different models \n",
        "model_params = EnsembleParams(\n",
        "    [\n",
        "     BaseModelParams('arima' , arima.ARIMAParams(p = 1 , d=1 , q=1)) , \n",
        "     BaseModelParams('sarima' ,\n",
        "                     sarima.SARIMAParams(\n",
        "                         p = 2 , d= 2 , q =1 , trend = 'ct' , \n",
        "                     seasonal_order = (1, 0 ,1 ,12) , enforce_invertibility = False , \n",
        "                     enforce_stationarity = False),\n",
        "     ),\n",
        "     BaseModelParams('prophet' , prophet.ProphetParams()) , \n",
        "     BaseModelParams('linear' , linear_model.LinearModelParams()) , \n",
        "     BaseModelParams('quadratic' , quadratic_model.QuadraticModelParams()),\n",
        "     BaseModelParams('theta' , theta.ThetaParams()), \n",
        "    ]\n",
        ")"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyE-sW09bWqJ"
      },
      "source": [
        "# Creating KatEnsembleParam with detailed configuration \n",
        "KatEnsembleParams = {\n",
        "    'models': model_params , \n",
        "    'aggregation': 'median' , \n",
        "    'seasonality_length': 7 , \n",
        "    'decomposition_method': 'multiplicative'\n",
        "}"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlX7gB4DcbY1"
      },
      "source": [
        "# Creating a Time Series dataset \n",
        "bitcoin_ts = TimeSeriesData(value = bitcoin_prices.Price,\n",
        "                            time = bitcoin_prices.index , \n",
        "                            sort_by_time= True)"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQdLi-tGcL9t",
        "outputId": "5eeb20a3-6c11-495c-ac37-cea2faac5395"
      },
      "source": [
        "# Creating a KatEnsemble model (or) instantiating it \n",
        "ensemble_model = KatsEnsemble(\n",
        "    data = bitcoin_ts , \n",
        "    params = KatEnsembleParams\n",
        ")\n",
        "\n",
        "# Fitting the model \n",
        "ensemble_model.fit()"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/stattools.py:671: FutureWarning:\n",
            "\n",
            "fft=True will become the default after the release of the 0.12 release of statsmodels. To suppress this warning, explicitly set fft=False.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/arima_model.py:472: FutureWarning:\n",
            "\n",
            "\n",
            "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\n",
            "been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\n",
            "between arima and model) and\n",
            "statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n",
            "\n",
            "statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\n",
            "is both well tested and maintained.\n",
            "\n",
            "To silence this warning and continue using ARMA and ARIMA until they are\n",
            "removed, use:\n",
            "\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
            "                        FutureWarning)\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
            "                        FutureWarning)\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
            "\n",
            "No frequency information was provided, so inferred frequency D will be used.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/base/tsa_model.py:527: ValueWarning:\n",
            "\n",
            "No frequency information was provided, so inferred frequency D will be used.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/arima_model.py:472: FutureWarning:\n",
            "\n",
            "\n",
            "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\n",
            "been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\n",
            "between arima and model) and\n",
            "statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n",
            "\n",
            "statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\n",
            "is both well tested and maintained.\n",
            "\n",
            "To silence this warning and continue using ARMA and ARIMA until they are\n",
            "removed, use:\n",
            "\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
            "                        FutureWarning)\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
            "                        FutureWarning)\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/arima_model.py:472: FutureWarning:\n",
            "\n",
            "\n",
            "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\n",
            "been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\n",
            "between arima and model) and\n",
            "statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n",
            "\n",
            "statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\n",
            "is both well tested and maintained.\n",
            "\n",
            "To silence this warning and continue using ARMA and ARIMA until they are\n",
            "removed, use:\n",
            "\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
            "                        FutureWarning)\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
            "                        FutureWarning)\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
            "\n",
            "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters/model.py:429: FutureWarning:\n",
            "\n",
            "After 0.13 initialization must be handled at model creation\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters/model.py:922: ConvergenceWarning:\n",
            "\n",
            "Optimization failed to converge. Check mle_retvals.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/kats/models/theta.py:121: FutureWarning:\n",
            "\n",
            "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters/model.py:429: FutureWarning:\n",
            "\n",
            "After 0.13 initialization must be handled at model creation\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters/model.py:922: ConvergenceWarning:\n",
            "\n",
            "Optimization failed to converge. Check mle_retvals.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/kats/models/theta.py:121: FutureWarning:\n",
            "\n",
            "`rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/arima_model.py:472: FutureWarning:\n",
            "\n",
            "\n",
            "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\n",
            "been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\n",
            "between arima and model) and\n",
            "statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n",
            "\n",
            "statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\n",
            "is both well tested and maintained.\n",
            "\n",
            "To silence this warning and continue using ARMA and ARIMA until they are\n",
            "removed, use:\n",
            "\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
            "                        FutureWarning)\n",
            "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
            "                        FutureWarning)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<kats.models.ensemble.kats_ensemble.KatsEnsemble at 0x7f5112273d50>"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2-IfczBc-pX"
      },
      "source": [
        "# Making prediction for the next 30 days \n",
        "forecast = ensemble_model.predict(steps = 30) "
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "E0uPDVYKdACp",
        "outputId": "83caf929-4a27-452b-a073-39eda30b7b20"
      },
      "source": [
        "# Aggregate individual model results (we will get the predictions for 30 Days)\n",
        "ensemble_model.aggregate()"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>fcst</th>\n",
              "      <th>fcst_lower</th>\n",
              "      <th>fcst_upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-19</td>\n",
              "      <td>42682.033343</td>\n",
              "      <td>38885.538435</td>\n",
              "      <td>44922.406883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-20</td>\n",
              "      <td>42073.837817</td>\n",
              "      <td>38406.420610</td>\n",
              "      <td>44889.006611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-05-21</td>\n",
              "      <td>41435.525715</td>\n",
              "      <td>37349.861803</td>\n",
              "      <td>44081.146809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-05-22</td>\n",
              "      <td>42372.449040</td>\n",
              "      <td>38075.124972</td>\n",
              "      <td>45665.318988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-05-23</td>\n",
              "      <td>42414.698328</td>\n",
              "      <td>37938.263973</td>\n",
              "      <td>45973.660794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-05-24</td>\n",
              "      <td>41846.072376</td>\n",
              "      <td>37104.568033</td>\n",
              "      <td>45932.084718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021-05-25</td>\n",
              "      <td>40862.114676</td>\n",
              "      <td>35997.432699</td>\n",
              "      <td>45016.413893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-05-26</td>\n",
              "      <td>41480.060326</td>\n",
              "      <td>36543.312256</td>\n",
              "      <td>46304.086241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2021-05-27</td>\n",
              "      <td>40962.347097</td>\n",
              "      <td>35964.142733</td>\n",
              "      <td>46216.077329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2021-05-28</td>\n",
              "      <td>40474.565032</td>\n",
              "      <td>35320.389075</td>\n",
              "      <td>45342.487118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2021-05-29</td>\n",
              "      <td>41293.389751</td>\n",
              "      <td>36112.047001</td>\n",
              "      <td>46942.808074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021-05-30</td>\n",
              "      <td>41405.318369</td>\n",
              "      <td>36041.372511</td>\n",
              "      <td>47154.853220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2021-05-31</td>\n",
              "      <td>41234.926705</td>\n",
              "      <td>35414.819467</td>\n",
              "      <td>47151.659016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2021-06-01</td>\n",
              "      <td>40704.306803</td>\n",
              "      <td>34423.904104</td>\n",
              "      <td>46177.863404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2021-06-02</td>\n",
              "      <td>41331.363397</td>\n",
              "      <td>35139.786268</td>\n",
              "      <td>47248.512299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2021-06-03</td>\n",
              "      <td>41191.421801</td>\n",
              "      <td>35001.974606</td>\n",
              "      <td>47332.845269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2021-06-04</td>\n",
              "      <td>40800.244025</td>\n",
              "      <td>34843.088881</td>\n",
              "      <td>46315.547180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2021-06-05</td>\n",
              "      <td>41652.352179</td>\n",
              "      <td>35419.608320</td>\n",
              "      <td>48014.479939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2021-06-06</td>\n",
              "      <td>41841.715299</td>\n",
              "      <td>35424.215241</td>\n",
              "      <td>47876.952866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2021-06-07</td>\n",
              "      <td>41714.089318</td>\n",
              "      <td>35306.987226</td>\n",
              "      <td>48039.497012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2021-06-08</td>\n",
              "      <td>41215.961246</td>\n",
              "      <td>35131.627620</td>\n",
              "      <td>46975.012645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2021-06-09</td>\n",
              "      <td>41884.488527</td>\n",
              "      <td>35425.986177</td>\n",
              "      <td>47842.978347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2021-06-10</td>\n",
              "      <td>41769.866631</td>\n",
              "      <td>35470.578074</td>\n",
              "      <td>48019.984275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2021-06-11</td>\n",
              "      <td>41206.294717</td>\n",
              "      <td>35277.111413</td>\n",
              "      <td>47115.723018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2021-06-12</td>\n",
              "      <td>42273.524324</td>\n",
              "      <td>35754.302566</td>\n",
              "      <td>48376.336005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2021-06-13</td>\n",
              "      <td>42474.722854</td>\n",
              "      <td>36099.065313</td>\n",
              "      <td>48640.835656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2021-06-14</td>\n",
              "      <td>42348.441880</td>\n",
              "      <td>35827.027290</td>\n",
              "      <td>48788.428983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2021-06-15</td>\n",
              "      <td>41591.303804</td>\n",
              "      <td>35416.480870</td>\n",
              "      <td>47620.748960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2021-06-16</td>\n",
              "      <td>42512.725765</td>\n",
              "      <td>36323.868418</td>\n",
              "      <td>48846.410214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2021-06-17</td>\n",
              "      <td>42385.342710</td>\n",
              "      <td>36125.284808</td>\n",
              "      <td>48745.960681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         time          fcst    fcst_lower    fcst_upper\n",
              "0  2021-05-19  42682.033343  38885.538435  44922.406883\n",
              "1  2021-05-20  42073.837817  38406.420610  44889.006611\n",
              "2  2021-05-21  41435.525715  37349.861803  44081.146809\n",
              "3  2021-05-22  42372.449040  38075.124972  45665.318988\n",
              "4  2021-05-23  42414.698328  37938.263973  45973.660794\n",
              "5  2021-05-24  41846.072376  37104.568033  45932.084718\n",
              "6  2021-05-25  40862.114676  35997.432699  45016.413893\n",
              "7  2021-05-26  41480.060326  36543.312256  46304.086241\n",
              "8  2021-05-27  40962.347097  35964.142733  46216.077329\n",
              "9  2021-05-28  40474.565032  35320.389075  45342.487118\n",
              "10 2021-05-29  41293.389751  36112.047001  46942.808074\n",
              "11 2021-05-30  41405.318369  36041.372511  47154.853220\n",
              "12 2021-05-31  41234.926705  35414.819467  47151.659016\n",
              "13 2021-06-01  40704.306803  34423.904104  46177.863404\n",
              "14 2021-06-02  41331.363397  35139.786268  47248.512299\n",
              "15 2021-06-03  41191.421801  35001.974606  47332.845269\n",
              "16 2021-06-04  40800.244025  34843.088881  46315.547180\n",
              "17 2021-06-05  41652.352179  35419.608320  48014.479939\n",
              "18 2021-06-06  41841.715299  35424.215241  47876.952866\n",
              "19 2021-06-07  41714.089318  35306.987226  48039.497012\n",
              "20 2021-06-08  41215.961246  35131.627620  46975.012645\n",
              "21 2021-06-09  41884.488527  35425.986177  47842.978347\n",
              "22 2021-06-10  41769.866631  35470.578074  48019.984275\n",
              "23 2021-06-11  41206.294717  35277.111413  47115.723018\n",
              "24 2021-06-12  42273.524324  35754.302566  48376.336005\n",
              "25 2021-06-13  42474.722854  36099.065313  48640.835656\n",
              "26 2021-06-14  42348.441880  35827.027290  48788.428983\n",
              "27 2021-06-15  41591.303804  35416.480870  47620.748960\n",
              "28 2021-06-16  42512.725765  36323.868418  48846.410214\n",
              "29 2021-06-17  42385.342710  36125.284808  48745.960681"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "7cmueWTgdzE_",
        "outputId": "0b7d36cc-b50e-4159-a3f8-19df92016aa6"
      },
      "source": [
        "# Plotting the model \n",
        "ensemble_model.plot()"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAGoCAYAAABbtxOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU9b3/8fckk5CwhQQEQoJsExEoiAoCtlqEm4JUwYUiagELij/sFavWqhVtsS5gsW5FvUHUoFW0LkRZAm7F5QoRUK7KhUZZJAsQIBCWkMnMnN8fuec4k0yWSWYyk8zr+Xj48Mx3zjnznS+0vvPN53y/NsMwDAEAAACQJMWEuwMAAABAJCEgAwAAAF4IyAAAAIAXAjIAAADghYAMAAAAeLGHuwPNrUuXLurdu3e4u9FolZWViouLC3c3WgzGKzCMV+AYs8AwXoFhvALDeAWG8ZJ2796tgwcP1miPuoDcu3dvbdq0KdzdaLSioiL16NEj3N1oMRivwDBegWPMAsN4BYbxCgzjFRjGSxo2bJjfdkosAAAAAC8EZAAAAMALARkAAADwQkAGAAAAvBCQAQAAAC8EZAAAAMALARkAAADwQkAGAAAAvBCQAQAAAC8EZAAAAMALARkAAADwQkAGAAAAvBCQAQAAAC8EZAAAAMALARkAAABBYRiG3G53uLvRZARkAAAABMUVV1whu92ukydPhrsrTUJABgAAQFCsWLFCknTixIkw96RpCMgAAAAIKqfTGe4uNAkBGQAAAEFVUVER7i40CQEZAAAAQUVABgAAALwQkAEAAAAvp06dCncXmoSADAAAgKBiBhkAAADwQkAGAAAAvBCQAQAAAC8EZAAAAMALARkAAADwQkAGAABA1DMMwzomIAMAACBqbN26VT169NCBAwd82t1ut3VMQAYAAEDUeOyxx1RcXKyVK1f6tBOQG+jIkSOaPHmyzjzzTA0YMECff/65Dh8+rMzMTGVkZCgzM1OlpaWSqqbl586dK4fDoSFDhmjLli3WfbKzs5WRkaGMjAxlZ2db7Zs3b9bgwYPlcDg0d+5cn6l9AAAABF9SUpIk6ejRoz7t3gHZ5XI1a5+CLaQB+ZZbbtH48eO1fft2bd26VQMGDNCCBQs0duxY5efna+zYsVqwYIEkac2aNcrPz1d+fr6ysrI0Z84cSdLhw4c1f/58bdy4UXl5eZo/f74VqufMmaMlS5ZY1+Xm5oby6wAAAES9Tp06Sao7IHs8nmbtU7CFLCAfPXpUH3/8sWbNmiVJio+PV6dOnZSTk6MZM2ZIkmbMmKEVK1ZIknJycjR9+nTZbDaNHDlSR44cUXFxsdauXavMzEylpKQoOTlZmZmZys3NVXFxscrKyjRy5EjZbDZNnz7duhcAAABCIy4uTlLNWWLvgOx93BLZQ3XjXbt26bTTTtNvfvMbbd26Veeee66eeOIJ7d+/X6mpqZKk7t27a//+/ZKkwsJC9ezZ07o+PT1dhYWFdbanp6fXaPcnKytLWVlZkqR9+/apqKgo6N+3uZSUlIS7Cy0K4xUYxitwjFlgGK/AMF6BYbwC09jxKisrkyQdO3bMJ1Pt2LHDOj5y5EiLzlshC8gul0tbtmzRU089pREjRuiWW26xyilMNptNNpstVF2wzJ49W7Nnz5YkDRs2TD169Aj5Z4ZSS+9/c2O8AsN4BY4xCwzjFRjGKzCMV2AaM14dOnSQJHXs2NHn+ry8POu4Xbt2LfrPImQlFunp6UpPT9eIESMkSZMnT9aWLVvUrVs3FRcXS5KKi4vVtWtXSVJaWpr27t1rXV9QUKC0tLQ62wsKCmq0AwAAoPmdOHHCOm7pJRYhC8jdu3dXz549ren2Dz74QAMHDtTEiROtlSiys7M1adIkSdLEiRO1bNkyGYahDRs2KCkpSampqRo3bpzWrVun0tJSlZaWat26dRo3bpxSU1PVsWNHbdiwQYZhaNmyZda9AAAAEBrmqmHVqwCOHz9uHbf0h/RCVmIhSU899ZSuvfZaOZ1O9e3bVy+88II8Ho+mTJmipUuXqlevXnr99dclSRMmTNDq1avlcDjUtm1bvfDCC5KklJQU3XvvvRo+fLgk6b777lNKSook6emnn9Z1112n8vJyXXzxxbr44otD+XUAAABQjcvl0jfffOMTkFv6DHJIA/LQoUO1adOmGu0ffPBBjTabzabFixf7vc/MmTM1c+bMGu3Dhg3TN9980/SOAgAAoFH++te/6o9//KP1un379j4B+fjx4zp16pS6dOkSju41CjvpAQAAIGDHjh2T0+n0CceJiYmKi4vzKbEYPHiwTjvttHB0sdFCOoMMAACA1sWsQX788ce1fft2tW/f3iqviIuLU0xMjM8M8u7du8PRzSZhBhkAAACNkpubq4SEBOu1YRiKjY1t8Q/pEZABAADQYOYMsikmJsbnveozyKbKysqQ9y1YCMgAAABoNO/l3jp27KjY2Fi/Adl7neRIR0AGAABAg3nPIJ9xxhk+Ablbt24+JRaHDh2y3nO5XM3XySYiIAMAAKDB7r//fp/X3rPFdrvdKrHYs2ePz9JuBGQAAAC0em6326e22GazWSUW1feqICADAACgVXC5XDpy5Ijf9zwej5xOpwYNGmS1xcTEyOPx1LiGgAwAAIBWYdasWUpOTrZqj9u3b2+953a75XQ61bVrV0m+M8jeq1tIBGQAAAC0EsuWLZMk68E774fyXC6XXC6X4uLirPfMh/SqB2SWeQMAAECrYj6M53Q6rbZTp05Jqno4T6oKyDExMVq/fn2NzUK8r4t0BGQAAADUyyyR8C6VKC8vlyRrN702bdro66+/1sGDB/XDDz/4XH/LLbc0U0+bzh7uDgAAACDyud1uGYbhs6ybGZAvuOACORwO3XzzzerZs6ekmjPG69evb77ONhEBGQAAAPVyu90+4bhLly46ePCgJOnAgQNauHChz/ktqea4OkosAAAAUC+Xy2UF5Iceeki//vWvrfcOHz5sHWdlZUmSKioqmreDQURABgAAQL3cbrdVf2y32xUbG+v3vKSkJEkt66G86gjIAAAAqJe5pJtUd0A2V7SoPoM8ZsyY0HYwiAjIAAAAqFddM8jmJiLme1LNGeRzzjmnGXoZHARkAAAA1KuugHz11Vdbx2ZA3r17d43rWwoCMgAAAOpVW4lFbGysRo8ebZ1n7qr3wQcf+FxPQAYAAECrUn0G2dxy2nvrafO92q5vKQjIAAAAqFf1gLxkyRJJvjvrme9VZ7PZCMgAAABoXaqXWBQWFvo9z19AjouLk8fjCWn/gomADAAAgHq53W5rdzy73a74+Hi/5/kLyPHx8cwgAwAAoHVxuVxWQI6Li9Pbb7/t9zzzIT1vBGQAAAC0Ct5lEYZhWAE5Pj5evXv39ntNbSUWLSkg+3/MEAAAAFHPOyB7PB4r5MbFxfmdKZZaR0BmBhkAAAB+eYda7xnkQAMyJRYAAABoFarPIHuXWNS23nH14FxcXKw2bdoQkAEAANDyeYdaj8cjp9MpKbAZ5O7duys2NpaADAAAgJavtof0GhqQf//730uq2o6adZABAADQ4lWfQQ40II8aNUqSFBMTwwwyAAAAWr7qM8hmiUV8fLwSEhL8XuMdkGNiqqImJRYAAABoFRozg+zdTkAGAABAq1LbKha1hWOJGWQAAAC0Yq+//rp1XH0nPZMZgk2tISCzkx4AAAD8WrVqlXVcfZk3Sdq1a5cSExN9rvEOzARkAAAAtCopKSnWsb8Si969e9d5vXdANsN1S0CJBQAAAPwaO3asdVx9HeSGaKkzyARkAAAA+OVdLuGvxKKh18fExLBRCAAAAFo+71lfcwbZbrfLZrM16HpmkAEAANCq+FsHuaGzxxIBGQAAAK2Md1nEjh07tGjRIp9l3OpjzjQTkAEAANAqeIfau+++W5J07NixBl/PDLIfvXv31uDBgzV06FANGzZMknT48GFlZmYqIyNDmZmZKi0tlVRV1zJ37lw5HA4NGTJEW7Zsse6TnZ2tjIwMZWRkKDs722rfvHmzBg8eLIfDoblz58owjFB+HQAAgKjS1FBLQK7FRx99pK+++kqbNm2SJC1YsEBjx45Vfn6+xo4dqwULFkiS1qxZo/z8fOXn5ysrK0tz5syRVBWo58+fr40bNyovL0/z58+3QvWcOXO0ZMkS67rc3NxQfx0AAICo0dhQO3jwYEkE5AbLycnRjBkzJEkzZszQihUrrPbp06fLZrNp5MiROnLkiIqLi7V27VplZmYqJSVFycnJyszMVG5uroqLi1VWVqaRI0fKZrNp+vTp1r0AAADQdI0NtbGxsZJ+DMgnT57U999/H7R+hVpId9Kz2Wz6xS9+IZvNphtvvFGzZ8/W/v37lZqaKknq3r279u/fL0kqLCxUz549rWvT09NVWFhYZ3t6enqNdn+ysrKUlZUlSdq3b5+KioqC/l2bS0lJSbi70KIwXoFhvALHmAWG8QoM4xUYxiswDRkv87f21dWXpcxgffDgQRUVFVmTmBs3bvTJdZEqpAH5008/VVpamg4cOKDMzEydeeaZPu/bbLYGr6PXFLNnz9bs2bMlScOGDVOPHj1C/pmh1NL739wYr8AwXoFjzALDeAWG8QoM4xWY+sarXbt2jbouISFBktS5c2efc7t169Yi/oxCWmKRlpYmSeratasuv/xy5eXlqVu3biouLpYkFRcXq2vXrta5e/futa4tKChQWlpane0FBQU12gEAABAc/kosGrLMm1laYS4T9+c//7nW+0WikAXkEydOWMuAnDhxQuvWrdNPfvITTZw40VqJIjs7W5MmTZIkTZw4UcuWLZNhGNqwYYOSkpKUmpqqcePGad26dSotLVVpaanWrVuncePGKTU1VR07dtSGDRtkGIaWLVtm3QsAAABN5y/Qem8/XRuzBtm8/owzzpAkVVZWBrF3oROyEov9+/fr8ssvlyS5XC5dc801Gj9+vIYPH64pU6Zo6dKl6tWrl15//XVJ0oQJE7R69Wo5HA61bdtWL7zwgiQpJSVF9957r4YPHy5Juu+++5SSkiJJevrpp3XdddepvLxcF198sS6++OJQfR0AAICo8+ijj9Zoczqd9V5XfQbZnHV2uVxB7F3ohCwg9+3bV1u3bq3R3rlzZ33wwQc12m02mxYvXuz3XjNnztTMmTNrtA8bNkzffPNN0zsLAACAGk6ePNmo68yAbM4gm9tTt5SAzE56AAAACCqzxKKhM8jOysiqTSYgAwAAoIbadigeM2ZMvddWn0E2A7K/GuRDR08p/4ejERWSCcgAAACoobYH6rz3oajNVVddJUnKyMiQVHeJxf5D5aqodMvt9h/Iw4GADAAAgBpqK4fo0KFDvddef/31Ki8vV+/evSXVXWLhcnvk9jS+n6FAQAYAAEANtc0gP/DAA/Vea7PZrM1CpLpLLNxujwxP5MweSwRkAAAA+LF9+3ZJ0o033qjLLrvMau/UqVPA9/I3g+ysdGvH7lIZktxGZE0hE5ABAABQw5QpUyRJp06dUlJSkqTat56uj78a5PIKt06ccslZ6VGE5WMCMgAAAGo6ePCgpKpQbM4AT58+vVH38jeDHBtjU6XLo0qXR55aVswIFwIyAAAAaigvL5dUtXOeua6x+e9A+atBNgxDHsOQ220owvIxARkAAAA1mWHY6XRaAddc3zhQ/kosCkpOyO025DE8io+LrEgaWb0BAABARDDrjlNSUpockKuXWBSXnFD5KZfcbkNujyGbzRaEHgcPARkAAAA1nH/++ZKkBx980Aq4TS2xMAPyobJTclZ65PYY6tS+TRB6G1wEZAAAAPgwDEPvvvuuJKlt27batm2bJOnbb79t1P2q1yB7PFXrH0uGktrHN73DQUZABgAAgI/qG3rk5ub6/DtQ1WuQDcOQKwIfzjMRkAEAAOCjtl30Glsr7F1i4XZ7qjYH8RgRt7ybiYAMAAAAH7UFZHMmOFDeJRalx5yqrPTI7fEotXPbRvcxlAjIAAAA8FFbQDaDbqC8Z5CPHKuQy+VR106Jio9r3EN/oUZABgAAgI/qAXnw4MGSpMTExEbdz5x5dlZW6nh5pVweQ3Z75MbQyO0ZAAAAwqJ6QF6+fLmkqm2nG8NcP9nprFSlq2r3vAhb+tgHARkAAAA+qgfkjh07SvLdCS8QNptNdrtdTmelPJ6q+uNIRkAGAACAj+oB2awhdrvdjb5nXFyc3C6XPG5DHo+hmJjInUImIAMAAMCHGZBfeeUVSVLnzp3Vrl07LVq0qNH3tNvtqnS5rPrjmAiusWjco4gAAABotcyAbNYcx8XF6fjx4026p91uV2nZSRmGoc5JCU3uYygxgwwAAAAfZq1xY5d188esQTYMKSE+Mpd3MxGQAQAA4MPzfw/RmatPBENcXJxcLlfEbi/tjYAMAAAAH8b/pdhgBmS73S63yyVDkZ+QCcgAAADwYc4g24L4IJ3dbpfL5ZLbTUAGAABACxOKGeS4uDi53C4ltons+mOJgAwAAIBqQjWDbHjcSmofH7R7hgoBGQAAAD5CV4Pc+I1GmhMBGQAAAD5CsYpFVQ1yZf0nRgACMgAAAHyEosTCrEFuCdhJDwAAAD5CUWIxefJkHS4jIAMAAKAFCsUM8vjLf6ONXx8I2v1CiRILAAAA+AjFDPL6TUV684OdPm0ew9Dm/y1Rwf7jQfucYCAgAwAAwEcoZpDbt43TKadbb7z/Y0j+eHOxnluxXYtf/9YK5ZGAgAwAAAAfoZhBbt82TpL0yVf7rLYdPxyxjssrImcJOAIyAAAAfIRiBrldYlyd77dNiJxH4wjIAAAA8BGSrabtvvd6JTdfX+04FLT7BxMBGQAAAD5CsVFIbMyPs9Eej6FPvtxXx9nhRUAGAACAj1CUWNhjf4ydr+R+5/PeqCHdgvY5wUBABgAAgI9QlFgMyUixjj/b6jt7PPHnvYP2OcFAQAYAAICPUMwgd2gXr5+e1b1Gu3fpRaSInMcFAQAAEBFCMYMsSW6P71rH48/vqV/+7HQdL4+sLaiZQQYAAICPUMwgS9J5g07zeX1acoJPbXKkiLweAQAAoFl8+umnysnJqdEeqhnkjNM7WceDHSkaNTiyHs4zhTwgu91unX322brkkkskSbt27dKIESPkcDh01VVXyel0SpIqKip01VVXyeFwaMSIEdq9e7d1j4cfflgOh0P9+/fX2rVrrfbc3Fz1799fDodDCxYsCPVXAQAAaFUuuOACXXbZZTXaf/jhB0nBn0GWpG4pibJJuulXg0Jy/2AIeUB+4oknNGDAAOv1nXfeqVtvvVXfffedkpOTtXTpUknS0qVLlZycrO+++0633nqr7rzzTknStm3btHz5cn377bfKzc3VTTfdJLfbLbfbrd/+9rdas2aNtm3bpldffVXbtm0L9dcBAABodcwJS9Ndd90lKfgzyJJ053VD9eQffhr0+wZTSANyQUGBVq1apeuvv15S1XT9hx9+qMmTJ0uSZsyYoRUrVkiScnJyNGPGDEnS5MmT9cEHH8gwDOXk5Gjq1Klq06aN+vTpI4fDoby8POXl5cnhcKhv376Kj4/X1KlT/f6KAAAAAHU7deqU3/ZQzPDGxtgisu7YW0hXsfjd736nRx55RMeOHZMkHTp0SJ06dZLdXvWx6enpKiwslCQVFhaqZ8+eVZ2y25WUlKRDhw6psLBQI0eOtO7pfY15vtm+ceNGv/3IyspSVlaWJGnfvn0qKioK8jdtPiUlJeHuQovCeAWG8QocYxYYxiswjFdgGK/AeI/XDz/8oJSUFL/nJCYmBu0zSw8dk8fjqbHt9LFyl/btq1Cb+NigfVZThCwgr1y5Ul27dtW5556rf/3rX6H6mAaZPXu2Zs+eLUkaNmyYevToEdb+NFVL739zY7wCw3gFjjELDOMVGMYrMIxX4yQnJ/sdu9TU1KCO6ZFTpfJ4PIqP8w3CMced6t49RYkJkbECcch68dlnn+mdd97R6tWrderUKZWVlemWW27RkSNH5HK5ZLfbVVBQoLS0NElSWlqa9u7dq/T0dLlcLh09elSdO3e22k3e19TWDgAAgIarXoNsitSH6EItZAUgDz/8sAoKCrR7924tX75cY8aM0T/+8Q9ddNFFeuONNyRJ2dnZmjRpkiRp4sSJys7OliS98cYbGjNmjGw2myZOnKjly5eroqJCu3btUn5+vs477zwNHz5c+fn52rVrl5xOp5YvX66JEyeG6usAAAC0WrUF5FA8pNcSNPs89sKFCzV16lTNmzdPZ599tmbNmiVJmjVrlqZNmyaHw6GUlBQtX75ckjRo0CBNmTJFAwcOlN1u1+LFixUbWzUt//e//13jxo2T2+3WzJkzNWjQoOb+OgAAAC1eRUWF3/ZonUFuloA8evRojR49WpLUt29f5eXl1TgnISFB//znP/1ef8899+iee+6p0T5hwgRNmDAhqH0FAACINrXNILvd7mbuSWSIznlzAAAAWGoLyC6XK6if01ImpAnIAAAAUc57B2NvwQ7IcbEx8niMoN4zFAjIAAAAUe7aa6/VwYMHa7SfdtppQf2cuLgYuVtAQI6MxeYAAADQrAzDN6gePnxYXbp0kSQNHTpUqampSk5ODupn2mNt8hiRH5CZQQYAAIhCHo+n1tdut1sJCQlB/0ybzaYT5cEt2wgFAjIAAEAUqr5ChfeMcmVlpbWsbjA5K93yeGrOXkcaAjIAAEAUqh6QvWeQjx07po4dOwb9M2NjYhRntynC8zEBGQAAIBpVD8iVlZWSqjYNKSwsDEmJhSFDdnsMM8gAAACIPNWXcDPXQn7ttdckSa+++mrQP9NmsynOHqNIX8iCgAwAABCFqj+kZwZkcyWLRx99NOifmdqlrWJjbMwgAwAAIPLUNoNstg8ZMiTon2mPjVFiGzszyAAAAIg81WuQy8vLJf1YixwXFxeSz41hBhkAAACRqHpAvuSSS1RSUtIMAVkRv900ARkAACAKVQ/IUtVuemZAjo+PD8nnlp9y6+DRUyG5d7AQkAEAAKJQ9RpkqerBPbMWOVQzyDabTXHVNiFxVrrljqCyCwIyAABAFKq+ioVUtQZyqEssbDYpzm5TeUVVQD963Kk4e2RF0sjqDQAAAJqFvxlkp9MZ8oCc1D5eiQl2lR2vmqk+Ue6SPTZG7RND83mNQUAGAACIQv5qkJsjIHdNSVRsjE0xMTZJkt1ukz3WFpLPaiwCMgAAQBTyF5Cbo8QixmaTPTZGNptNHsNQ7P+9jiSR1RsAAAA0i/pKLEK1ikVMjE0xsTZVujzad7BciQl2ZpABAAAQfv4e0vMOyLHVVpoIppQObRRjsympfZzaJtjVv3dyyD6rMQjIAAAAUcjfDPKhQ4d0//33S6paji1UOraLV1xcjBLa2NUnrWPIPqexCMgAAABRyF8N8pNPPtksn22Lkdom2NUuwR5xS7xJBGQAAICo5C8gt2vXrtk+3x4bo57d2zfb5wWCgAwAABCF/AXkUNYde7PJppiY0JZxNAUBGQAAIAr5q0FuLm3iY9Wre4ewfX59CMgAAABRyFzFYtWqVdq8ebMkyTCMZvv8xAR7s31WoCK3ZwAAAAgZcwa5Z8+e6tixaiUJf2UX0YgZZAAAgChkhmG73a6YmKpIWFFREc4uRQwCMgAAQBQyA3JsbKz1cN6WLVvC2aWIQUAGAACIQmaJhfcMMqowGgAAAFHo008/lURA9ofRAAAAiEJvvvmmpKoSi+oB+fvvvw9HlyIGARkAACCK+ZtB7tOnT5h6ExkIyAAAAFHM+yE9U6TucNdcCMgAAABRjBrkmhgNAACAKOavBjnasZMeAABAFLPbiYPV1fvjwlNPPaXS0tLm6AsAAACamb8a5GhXb0Dev3+/hg8frilTpig3N1eGYTRHvwAAANAMqEGuqd7ReOCBB5Sfn69Zs2bpxRdfVEZGhv74xz9G/fp4AAAArUFMTAwBuZoGjYbNZlP37t3VvXt32e12lZaWavLkyfrDH/4Q6v4BAAAgREaNGiWJZd2qq7cq+4knntCyZcvUpUsXXX/99frrX/+quLg4eTweZWRk6JFHHmmOfgIAACBIPB6PJGncuHGSCMjV1RuQDx8+rLfeeku9evXyaY+JidHKlStD1jEAAACEhtvtlsQKFrWpd1Tmz59f63sDBgwIamcAAAAQei6XS5JYvaIWIavIPnXqlM477zydddZZGjRokP70pz9Jknbt2qURI0bI4XDoqquuktPplCRVVFToqquuksPh0IgRI7R7927rXg8//LAcDof69++vtWvXWu25ubnq37+/HA6HFixYEKqvAgAA0Kr4m0Hu0qVLuLoTcUIWkNu0aaMPP/xQW7du1VdffaXc3Fxt2LBBd955p2699VZ99913Sk5O1tKlSyVJS5cuVXJysr777jvdeuutuvPOOyVJ27Zt0/Lly/Xtt98qNzdXN910k9xut9xut377299qzZo12rZtm1599VVt27YtVF8HAACg1TADsvcMcklJifbu3audO3eGq1sRI2QB2WazqX379pKkyspKVVZWymaz6cMPP9TkyZMlSTNmzNCKFSskSTk5OZoxY4YkafLkyfrggw9kGIZycnI0depUtWnTRn369JHD4VBeXp7y8vLkcDjUt29fxcfHa+rUqcrJyQnV1wEAAGg1zBKL6jXI6enp6tOnTzi6FFFCuuid2+3W0KFD1bVrV2VmZqpfv37q1KmT9YeRnp6uwsJCSVJhYaF69uwpqeoPKykpSYcOHfJp976mtnYAAADUzd8MMn4U0kcXY2Nj9dVXX+nIkSO6/PLLtX379lB+XK2ysrKUlZUlSdq3b5+KiorC0o9gKCkpCXcXWhTGKzCMV+AYs8AwXoFhvALDeDXcvn37JEnHjx9v0bkoVJplbY9OnTrpoosu0ueff64jR47I5XLJbreroKBAaWlpkqS0tDTt3btX6enpcrlcOnr0qDp37my1m7yvqa29utmzZ2v27NmSpGHDhqlHjx6h+qrNoqX3v7kxXoFhvALHmAWG8QoM4xUYxqthzBnkzp07M2Z+hKzEoqSkREeOHJEklZeX67333tOAAQN00UUX6Y033pAkZWdna9KkSZKkiRMnKgGRpQIAACAASURBVDs7W5L0xhtvaMyYMbLZbJo4caKWL1+uiooK7dq1S/n5+TrvvPM0fPhw5efna9euXXI6nVq+fLkmTpwYqq8DAADQatRWg4wqIRuV4uJizZgxQ263Wx6PR1OmTNEll1yigQMHaurUqZo3b57OPvtszZo1S5I0a9YsTZs2TQ6HQykpKVq+fLkkadCgQZoyZYoGDhwou92uxYsXW/Uyf//73zVu3Di53W7NnDlTgwYNCtXXAQAAaDXMSUxzQQX4CllAHjJkiL788ssa7X379lVeXl6N9oSEBP3zn//0e6977rlH99xzT432CRMmaMKECU3vLAAAQBT5/vvvJVXlMtQU0lUsAAAAEHnMgNyvX78w9yQyEZABAACizL59+9SuXTt17Ngx3F2JSARkAACAKON0OtWmTZtwdyNiEZABAACiTGVlpeLi4sLdjYhFQAYAAIgyTqeTgFwHAjIAAECUqaysZA3kOhCQAQAAoozT6VR8fHy4uxGxCMgAAABRxul0MoNcBwIyAABAlOEhvboRkAEAAKIMD+nVjYAMAAAQZZhBrhsBGQAAIMqUlJQoKSkp3N2IWARkAACAKGIYhnbv3q3TTz893F2JWARkAACAKLJ9+3aVl5cTkOtAQAYAAIgin332mSSpX79+Ye5J5CIgAwAARJGKigpJ0sCBA8Pck8hFQAYAAIgilZWVksRGIXUgIAMAAEQRMyCz1XTtCMgAAABRxOl0SmIGuS4EZAAAgChiziCzUUjtCMgAAABRxOl0ym63y2azhbsrEYuADAAAEEXYZrp+BGQAAIAoUllZyQN69SAgAwAARBGn08kMcj0IyAAAAFGkoqJCbdq0CXc3IhoBGQAAIIqcPHlS7dq1C3c3IhoBGQAAIIqcOHFCbdu2DXc3IhoBGQAAIEpcffXVevfddwnI9SAgAwAARInly5dLEiUW9SAgAwAARBlmkOtGQAYAAIgyOTk54e5CRCMgAwAARAG3220djxs3Low9iXwEZAAAgChw8OBBSVK/fv30zjvvhLk3kY2ADAAAEAW+/vprSVJWVhZbTdeDgAwAABAFKioqJEkdOnQIc08iHwEZAAAgClRWVkoSs8cNQEAGAACIAk6nU5IUFxcX5p5EPgIyAABAFDADMjPI9SMgAwAARAECcsMRkAEAAKKAWYNMiUX9CMgAAABRgBnkhiMgAwAARAECcsMRkAEAAKIAJRYNR0AGAACIAizz1nAEZAAAgCjgdDoVExOj2NjYcHcl4hGQAQAAokBlZSX1xw0UsoC8d+9eXXTRRRo4cKAGDRqkJ554QpJ0+PBhZWZmKiMjQ5mZmSotLZUkGYahuXPnyuFwaMiQIdqyZYt1r+zsbGVkZCgjI0PZ2dlW++bNmzV48GA5HA7NnTtXhmGE6usAAAC0aE6nk4DcQCELyHa7XY8++qi2bdumDRs2aPHixdq2bZsWLFigsWPHKj8/X2PHjtWCBQskSWvWrFF+fr7y8/OVlZWlOXPmSKoK1PPnz9fGjRuVl5en+fPnW6F6zpw5WrJkiXVdbm5uqL4OAABAi+Z0Oqk/bqCQBeTU1FSdc845kqQOHTpowIABKiwsVE5OjmbMmCFJmjFjhlasWCFJysnJ0fTp02Wz2TRy5EgdOXJExcXFWrt2rTIzM5WSkqLk5GRlZmYqNzdXxcXFKisr08iRI2Wz2TR9+nTrXgAAAPDFDHLD2ZvjQ3bv3q0vv/xSI0aM0P79+5WamipJ6t69u/bv3y9JKiwsVM+ePa1r0tPTVVhYWGd7enp6jXZ/srKylJWVJUnat2+fioqKgv4dm0tJSUm4u9CiMF6BYbwCx5gFhvEKDOMVGMarbkePHlVsbKyVgxiv2oU8IB8/flxXXnmlHn/8cXXs2NHnPZvNJpvNFuouaPbs2Zo9e7YkadiwYerRo0fIPzOUWnr/mxvjFRjGK3CMWWAYr8AwXoFhvGpnt9uVkJDgM0aMl38hXcWisrJSV155pa699lpdccUVkqRu3bqpuLhYklRcXKyuXbtKktLS0rR3717r2oKCAqWlpdXZXlBQUKMdAAAAVbZv365x48apqKiIEosAhCwgG4ahWbNmacCAAbrtttus9okTJ1orUWRnZ2vSpElW+7Jly2QYhjZs2KCkpCSlpqZq3LhxWrdunUpLS1VaWqp169Zp3LhxSk1NVceOHbVhwwYZhqFly5ZZ9wIARIby8nJWGALCaMCAAVq3bp0eeeQRFRQU6LTTTgt3l1qEkAXkzz77TC+99JI+/PBDDR06VEOHDtXq1at111136b333lNGRobef/993XXXXZKkCRMmqG/fvnI4HLrhhhv09NNPS5JSUlJ07733avjw4Ro+fLjuu+8+paSkSJKefvppXX/99XI4HOrXr58uvvjiUH0dAECATp48qbZt2+qOO+4Id1eAqPT2229bx0888YQ2btyoYcOGhbFHLYfNiLIf7YcNG6ZNmzaFuxuNVlRURL1QABivwDBegWPManfw4EFrtsr8Tw3jFRjGKzCM148OHDigbt261Wh/9dVXNXXqVEmMl1R7LmQnPQBASLhcrnB3AYhaBw4c8Ns+YMCAZu5Jy0RABgCEhNPpDHcXgKg1ePBg63j48OHWcVJSUji60+IQkAEAIUFABiLDRRddZB136NAhjD1pOQjIAICQqKysDHcXgKjkdrt9XickJFjHBOSGISADAEKCGWQgPMrKynxet2vXTo899ph69erFOsgNREAGAIQEARkIjx9++MHnda9evfS73/1Ou3fvDk+HWiACMgAgJCixAMIjLy/P57W5azEajoAMAAgJ7xlkj8cTxp4A0eW7777zKaWg7jhwBGQAQEgUFhZax8wmA83n+PHj6tixo/WagBw4AjIAICS86x3ZNARoPsePH1e7du2s197HaBgCMgAgJE6ePGkdb9++PYw9AaLL8ePH1b59ez355JOSpM6dO4e5Ry0PARkAEBL/+Mc/rOPvv/9ekvTkk0/q97//fbi6BEQFMyDffPPNMgxDiYmJ4e5Si0NABgCExJ49e6zjNm3aSJIWLlyoRx99NFxdAqLCiRMnKKtoIgIyACDobDabz+vqO3sBCI1///vf+uyzz2S328PdlRaNgAwACDke0gOax+TJkyX9WNaExiEgAwBC5vHHH5fEDDLQXE6dOiVJOnToUJh70rIx/w4ACJnevXtLkq655hp98803VrthGDXKMAA0XZcuXZSfn6+77ror3F1p0ZhBBgCEjPduXg899JB1zMYhQPD9+9//1qZNmzRp0iTdeeed4e5Oi0ZABgAElWEY1rG5ekV1Bw8ebK7uAFHBMAz1799flZWVlDQFAQEZABBUHo/HOq4tIBcVFTVXd4Co4L0xD+seNx0BGQAQVN4rVtQWkI8cOdJc3QGiQkVFhXW8ZMmSMPakdSAgAwCCyjsge9cge/Oe7QLQdObqFZKUlJQUxp60DgRkAEBQedc/1jaDTEAGgss7IKPpCMgAgKBqSIkFARkILjMgP/bYY2HuSetAQAYABFVDSixKSkqaqztAVDBrkPv06RPmnrQOBGQAQFA1ZAb5rrvu0po1a5qrS0CrZ84g1/a/OQSGgAwACKqG1CBL0scff9wc3QGiwooVKyRJp512Wph70joQkAEAQeU9gxwbG6v+/fv7Pa9Tp07N1SWgVXv77bf1yCOPSJJ69uwZ5t60DgRkAEBQeW8jHRsbW+ssss1ma64uAa3aFVdcYR0zgxwcBGQAQFA5nU7ruK6AfPTo0ebqEhAVli9fzg+eQUJABgAElfeOXjExMXriiSf8nkdABppu27Zt1vFVV10Vxp60LgRkAEBQec8g22w2jRo1Sp9//rkk6fzzz9eHH34oiYAMNFVJSYkGDRokSZo+fXqYe9O62MPdAQBA62IG5HfffddqO++88zRv3jxdeeWVGjp0qM466ywCMtBERUVF1vEdd9wRxp60PswgAwCCyiyxSE5OttpiYmL0l7/8RV27dpUkJSQk+JRioOU5cOCADMMIdzeimrn28eLFi/WTn/wkzL1pXQjIAICgMmeQ61oDOS4uTuvWrdNrr73WXN1CEHzyySf6/vvvtXnzZnXr1k2vvvpquLvUahw7dkylpaUBXVNeXi5JGjBgQCi6FNUIyACAoDJnhmvbZlqS7PaqCr+pU6c2S58QHBdeeKEcDodVR75hw4Yw96j16NOnj1JSUgK6xpxBTkhICEWXohoBGQAQVCdPnpRU93+04+Limqs7CIGdO3dKIpgF06FDhwK+hoAcOgRkAEBQHThwQJKsemN/CMgtj/cW4s8++6wk310TparfHlxyySX6n//5n2btW0t3/Phx67i8vFwvv/yyZs2aVe91ZolFYmJiyPoWrQjIAICg2r9/v+Lj45WUlFTrOQTklsc7xJkee+wxn9dTpkzRqlWrdNZZZzVXt1osj8ej66+/XjabTR06dLDa27Ztq2nTpun555+v8yHIwsJC7dmzRxK754UCARkAEFT79+9X9+7d69zRy/u966+/vtbz3nrrLa1Zsyao/UPjlJWV+W33eDzW8TvvvNNc3WnxCgoKtHTp0jrP8fdDiVQ1c5yenq67775bXbt2VefOnUPRxahGQAYABNW+ffvUrVu3Os/xrrdcunSp1q1bZ71euXKlxowZI8MwdOWVV2rChAkh6ysa7tixYwG15+TkhLI7LV5xcXGNtt/97nc+rw8ePOj32ry8POu4Xbt2we0YJBGQAQBBZs4g1+WTTz7xef3Pf/7TOr7yyiv10Ucfae3atSHpHxqnthnk2jZ8+fLLL0PZnRbNMAyNHDnSp+3OO+/UokWLfNpKSkr8Xr9ixQrr+L777gt+B0FABgAEV3Fxcb0zyNXFxsZax+YDR//617+sNjakCL/aZor/8Y9/aPv27ZKkX/ziF+rYsaOkmg/w4Uc//PCDz2u3260FCxYoNjZWZWVlevvttyVVzSDn5eXJZrP51Hvv3btXDodDhmHouuuua86uRw0CMgAgaHbu3KkDBw6ob9++AV1nbi4i/TgjuXDhQquttnCG5vPdd9/5vE5NTZUk/fGPf7Q2qjhw4IB++tOfKiUlRV988YUeeughfrjxw3tFEKfTqZiYH+NYhw4dNHjwYElVAfnWW2+VJN122226/PLL9cILL2jlypUaOHBg83Y6yhCQAQBBYy7xNmTIkDrP+/LLL/Xoo4/q4osvliSdOHFCUu0zxY1ZIxbBlZ+fr7Zt21qvX3nlFZ/3PR6PduzYoQEDBqhTp05at26d7rnnHhUVFWnlypVKSkqy1u2NduZmOs8//7zfFV26dOkiqarEwnvMVqxYoZkzZ6qiokI9e/Zsns5GqZAF5JkzZ6pr164+e4MfPnxYmZmZysjIUGZmprWlomEYmjt3rhwOh4YMGaItW7ZY12RnZysjI0MZGRnKzs622jdv3qzBgwfL4XBo7ty5/IQKABHgkksukVT/uqxDhw7VbbfdptWrV+vss8+2NhepLQjX9jQ/mk9JSYnP2tbVd0rctWuXysvLNWDAAJ8Hxw4fPqybbrpJZWVlSkxMrDET3ZycTmdE/LBlht7ads7r2LGj4uLitH79ep9M5G38+PEh6x9CGJCvu+465ebm+rQtWLBAY8eOVX5+vsaOHasFCxZIktasWaP8/Hzl5+crKytLc+bMkVT1P6r58+dr48aNysvL0/z5861QPWfOHC1ZssS6rvpnAQCanxk+AtnZq23btlZALioq8nuOOcOM8CkpKdFpp52mOXPm6G9/+5tGjRrl8/6nn34qSRo5cqS+/vprq33IkCE+wTojI0NXXnmlT5lBc2nTpo26dOlS4yHRYMjNzdX777/foHPr2wHPZrOpsrJS7777riSpf//+1ntbtmzRzp07rR9GERohC8gXXnhhjZ+McnJyNGPGDEnSjBkzrKcwc3JyNH36dNlsNo0cOVJHjhxRcXGx1q5dq8zMTKWkpCg5OVmZmZnKzc1VcXGxysrKNHLkSNlsNk2fPt3niU4AQHi1adOmwee2bdvW2hGsekDu0aOHJAJyJDAD8tNPP61bb721xjrXGzdulCT169dPGRkZPu9t3rzZ5/Vbb72lX//616HtcDXbtm2zji+88MKg/526+OKLlZmZ2aBzA90ievXq1RowYICWLVums88+W3369Gl0P9EwzVqDvH//fquov3v37tq/f7+kqt1gvGtp0tPTVVhYWGd7enp6jXYAQMvz6aef6vPPP5fL5arx6+/LLrtMUmABec+ePdZvGxE8ZkCuzTPPPKOkpCQlJiZq69atWr9+vc/748eP1+jRo/XSSy9JkpYvX+6zyUio3XjjjT6vp0+f3uhZ7CeffNJnaUJv69ev1/fff1/n9WZArusHSXPW/b/+67/Ut29fbdu2TdOmTWtUfxE4e7g+2Gaz1bnLUjBlZWUpKytLUtUC9rX9Cq8lqG1NRPjHeAWG8QocY/Yj72dBiouL/f5/rb/x6t27t/73f/9X+fn5NZa/MmtZCwoKGvT/3SdOnNAZZ5yhK664Qk899VSgXyHiRMrfL4/Ho/3796tdu3Z1/jl06dLFet/hcKiwsFATJ07U5s2blZSUZO0cN23aNL300kv68ssvrYmzYKhrvHbt2uXz+q233tJHH30U8GoQlZWVuuWWWyRJycnJmjVrls8GH6NHj1b//v21cuVKffHFF7rwwgtr5J2tW7dKkmJiYmodz02bNunUqVP1jnlTRMrfr0jUrAG5W7duKi4uVmpqqoqLi62fjtLS0rR3717rvIKCAqWlpSktLc1nHcyCggKNHj1aaWlpKigoqHF+bWbPnq3Zs2dLkoYNG2b9yq6laun9b26MV2AYr8AxZlW8A8gFF1ygpKQkv+dVH6+5c+dqzpw5Sk5Otpa7Gj58uL744gudeeaZkqoeCGvIOJs7jL311lt68803G/U9Ik0k/P06cOCAnE6nzjzzTL/9sdlsMgxDaWlpNd43d4MbNWqU9d4VV1yhl156STExMTXOLykp0a9+9Ss988wz1vJxgah+v507d2r+/PkqLCy0grnp5MmTAY+v9w54paWlWrRokc8ybZK0Y8cOzZ07V2vWrNGqVas0YcIE7dy50/qt+N133y2p6u95c00W1iYS/n5FomYtsZg4caK1EkV2drYmTZpktS9btkyGYWjDhg1KSkpSamqqxo0bp3Xr1qm0tFSlpaVat26dxo0bp9TUVHXs2FEbNmyQYRhatmyZdS8AQHjs3r1bkvTRRx/VGo79MeswT506pbKyMtntdr3yyiu6//77rQeRbrjhhjpXP6ioqFCfPn00YsQIq8174gVNs2TJEklSr169fNrPOussJSQk6P7775ckvzW4v/rVryTJZ0MLc7k4s/bc27p167R+/XoNHDhQl19+eZPLZWbPnq1ly5ZJqnr+ad++fVq8eLGkqqXrAuVv1vWRRx6p0bZmzRpJ0r///W8dOnRI/fr104QJE7RhwwZJVZuqhDscow5GiEydOtXo3r27YbfbjbS0NOO5554zDh48aIwZM8ZwOBzG2LFjjUOHDhmGYRgej8e46aabjL59+xo/+clPjC+++MK6z9KlS41+/foZ/fr1M55//nmr/YsvvjAGDRpk9O3b1/jtb39reDyeBvXr3HPPDe4XbWaFhYXh7kKLwngFhvEKHGP2oxUrVhiSjC1bttR6jr/xevXVVw1JxsMPP2z853/+p5GcnGy9d+rUKUOS9U9tXnvtNZ/zJBldu3Zt2heKAJHy98sc02+//dan3eVyGRUVFcbRo0eN1atX+/1vscvlMo4fP+7T9vHHHxuSjPfee6/G+c8995zPn+O8efMa3E9/45WZmWnd68svv7TaExMTjTvuuMPYuHGjUVRU1ODPeP/992v8XTP/KSwsrPU9f+eGWyT0Idxqy4UhK7F49dVX/bZ/8MEHNdpsNpv101x1M2fO1MyZM2u0Dxs2TN98803TOgkACBpzBzxzq+GGMh9Uuvvuu3XBBRf4XF99rd2f/exnWr58uc+D2pLvr71N5qYlaBrDq7a8+g6JsbGxio2NVXx8vLXpS3WxsbE+6yJLdc8gV39Q84EHHtDUqVM1aNCggPqdl5envn37avfu3erbt6/69+/vs1xap06d9Pnnn+uvf/2rJkyYoFWrVjXovk8++aR1fN5551llPb/85S/Vo0cP7dmzRydPntSpU6d09913+12G9sorr6S0IcKxkx4AICjKysokBR6QvZe6+uSTT6w1kSXV+BX0Z5995ncHMe+HpC699FJJVb/uN9hEKmAul0t/+ctf9Pnnn+vIkSPauXOnpKqtpQNZ37ou5kYy3n/WJnPFEu8d5N54442A7v/oo49qxIgRmj17tr7//nv9+te/1urVq302sOnUqZO1dvPq1asbdN+VK1fqnXfesV4nJyfrvffe8znn9NNP15lnnqmhQ4dqyZIluummm/Twww9r9erVMgxDTqcz4O+D5kdABgAERWMDcvVd96rXeNZXz+wdsl5++WWtWLFCF1xwgd577z2f4IyG+fjjj3Xffffp/PPP19ixY/Xll19KkrVpRTCYM8hmGP7444+1Y8cOSVW7JrZr105t2rSx9k7485//HNA21b///e8lSW+//bY8Ho/1sKc377WaG7put/nb8auvvlpS1Q9wF110kW6//XarDttbenq6Fi9erLvuusuaYfe3tTQiDwEZABAUZWVlatOmTUCbhEhS586dfV5X30DC31a73uvXHj58WFLVkp7XXnutYmJirJUTnnzySWt31kBE88yzOWMsVY39nj17JKnG5h9N0a1bN0lV+yBUVlbq5z//uSZMmCCpKjSbJRkvvviiFZInT57c4PtXX/3CX99feeUV6zg5OblB9y0rK9OgQYP05JNPqnfv3vrzn/+s2NhYLVq0SOecc06D+4fIR0AGAATFu+++26in8qvvuvof//EfPq/79u2rkSNH+rR5L/VprlbhfZ+JEydax88++2xAgffAgQOKiYlRfHy83xrZ1qq8vFzr16+3dsSTpNNOO0379u1TQkKCOnToELTPSkxMVJcuXXTfffdZdeY7d+7U7Nmz9eGHH6p9+/bWueYzSmY5REMcO3bMZ9UMh8NR45x27dppz549uvHGG3Xs2LEG3be8vFydOnVSly5dtGvXLp9VU9C6EJABAE22efNmbdu2LaBfg5uqP6zUqVOnGueYS2OZzKW/CgsLdf7550uSz6/RFyxY4Pf8hjCXk6usrNRf/vKXBl/X0p133nkaPXq0nnvuOautpKREr732mgYMGBD0JcnMWX5vS5YsUX5+vk/ZTLt27XTDDTfI5XLV++Clx+PRqFGjVFBQILvdruLiYq1fv97v3ympql64e/fuOnHihFwuV719PnnyZI2SILROBGQAQJPs3btXw4YNkyQ9+OCDAV9vs9l8ViuqPlvsz7p16yRJzz//vNXmvUKBJP2///f/rOPCwsIG98dcjUNSvVsGtxaGYfisDDVo0CDNnTtXUtWfr78HI5vqhRde8Hk9ffp063jfvn0+7/3mN7/RiRMn9Nlnn9V5z02bNlk/TM2YMUPdu3fXhRdeWOc1Zo37TTfdVG+fy8vLrfpptG4EZABAkzz88MPW8Z133tmoe1RWVkqSFi5caNWnerv99tslSXZ71eqk5koC27dvlyQtW7bMes+0ePFia2vjH374QVlZWbrssst8amz98Z5tjpateKvP5p555pn69ttvrdc///nPg/6Z1113nQzD0PTp09WhQwc9++yzPqUx3swNSjZu3KgZM2b4Xf1C8i29+dnPftagfvzyl7+UVLWhR12OHz+ur776qsbSg2idCMgAgCYxa1NHjBih2NjYRt3D6XRKkk/tqbf77rtPkvTYY4/poosusmqKy8rKdPbZZ2vatGk1romJidG4ceMkVc1A3njjjcrJyfGZqazO4/FY69xOmjRJmzZt8lsK0NqYq0mY/va3v+ncc8+1Xl911VUh++zs7GyVlZUpMTFRjz76qN9zunbtKpvNpoULF2rZsmUaP368brjhBnk8Hnk8Hus8cz3sQGb+zzjjDJ155pnq2rVrneeZpTw//elPG3xvtFwEZABAo9x+++2y2WzWFtMrVqxo9L3qC8gdO3aUYRj6z//8T7Vv314FBQXasGGDz4oH/qSlpSklJcVnJthctsyf7Oxs6yG1efPm6dixY3I4HNYMt2nlypW65JJLGlS32hJ4P6Q2fvx4nX766XrwwQe1c+dOrVmzRmlpac3SD/NBy8GDB/u02+12nwctP/nkEz333HO69957FRsbq5UrV+qCCy7QQw89pE6dOtXY0KQ+cXFxNf4sT548qWHDhmnt2rX67rvv9PXXX0uSbr755sZ8NbQwBGQAQMAKCwv1t7/9TZL0+uuvKzY21m9pREOZAbmusGtq27atfvjhB40aNUofffRRvddMmjTJOp4/f75OnjxZa7A1d3tduHChhg0bpnHjxuno0aPWUnJSVdnFpZdeqlWrVlk/HLR03qFv4MCBkqpCaZ8+fTR+/Phm60dKSopWrlypjz76qEHnP/TQQ5KqNocxV7mor+bYH7vdXuOHoBtuuEGbN2/W+PHjrWXiFixY0OjfkqBlISADAAL21Vdf+bw2fwXeWBUVFZIaFpCPHz/u87q+JdzMkomnnnrK2sSktmW9SktLde655+oPf/iDJOnaa6+V9OMmKJJ8fhXfWmqU169fL0m6/PLLa6wA0tx++ctf1lgbW/px+b8ZM2Zo0aJFfstqpIbXHnvzN4Psr9aY7aGjBwEZABCwoqIiSbJqfJsaHMyA3JCtjM11j03VZ/6qa9++vVWeYdZLf/XVV1q5cqX1sNehQ4e0fv16HT161GfnPvPYXNmi+jJ23suOuVwun/f37NljbbIR6a655hpJVRtzROpOb2+88YZeeeUVvfDCC7r99tu12gnpMQAAHyZJREFUbNky6z2Px6NvvvlG7du31xVXXBHwvc0ZZLfbrVtvvVWvv/66XnzxxRrnDR06tClfAS0IARkAEDDzYaghQ4ZIqrnEWqDMEouGrBDwzjvv+KxP/Pjjjzf4c8xflY8ZM0aXXnqpnnvuOS1atEhdunTR6NGjlZeX5xOQzZnMkpISvfnmm9bSZ+Ysq3dAnjRpkhITE2Wz2dS5c2f17t3bWv7OdOzYsYjcpS8xMVE9evQIeJvw5pSUlKSrr77a728qbDabBg0apB07dqhfv34B3zsuLk6VlZXaunWrHn/8cb8PJS5fvrxGbTRaL3v9pwAA4KuoqEhdunSx1sdtyMxvXcwZ5IZsU92rVy/NmzdP8+bNC/hzLrjgAp/XBw4c8Fm7ubKy0ueBtD59+kiStQ2y6eabb9Zdd91l1SAXFhZq9erV1vtmzfLBgwc1c+ZMlZWVqUePHnrqqad06aWXWsvURYrjx4/X+oBkJPvDH/6gVatWNfk+drtd5eXlNUpmTp06JbfbrSNHjlBeEWWYQQYABGz79u1KTU21dihr6pbM55xzjqSqrY1Dqfrso7+NTc444wzrODU1tcb78+bNU9u2bdWpUyc99NBD6tevn9LT0yVJPXv21OOPP64BAwZY57/wwgt688039dRTT0mq2pL7T3/6U1C+T7AcPHiw1t3mItnChQt9NjhprLi4OBUWFtZ4ILFNmzZq27Yt4TgKEZABAA02bdo0PfHEE1q/fr0SEhKsbXebGpAXL16svLw8nX766cHoZp127NhRI1Rt27bNKqfwfvireqB+6KGHNH/+fEk/1u2OHDlS06ZN04MPPqg9e/bolltu0TPPPKNnnnlGBw4ckMfj0b333qv77rtPO3bskFRz6+xw+/bbb31CfbSx2+016sXNhwIRnSixAAA0yLFjx/Tyyy/r5ZdfllS12oC53qxZi9xYCQkJGj58eJP72BDmDPGzzz5rbUfdv39/bd68WceOHasxk/qvf/1LL774om6++WZrpluq2rTkwQcf9Dvz+vOf/9xn97n777/fOh47dmytq2iEw8qVK7Vv3z4NGjQo3F0JG38PJnrXoiP6EJABAPW64447tGjRIp+2n/3sZzrnnHOUl5ens88+O0w9a7wbb7xRl1xyiQoKChQTE2NtZ1xd9bBrio+Pb9S2wx06dND+/fsDvi4UTpw4oUsvvVSSdNlll4W5N+Fj/iZEqvoBZsKECbUuI4foQIkFAKBeb731Vo22UaNGSZKGDx8uu71lzrekpaVpxIgRzfqZHTp00DfffKN77723WT/XW0FBgUaOHKmcnBxJUrdu3awVPqKR92ojTz/9tG677baQ18MjshGQAQD1OuussyRVheL58+dr7969atu2bZh71TKZD4I98MADzfq5Ho9HL774ooqLi3XNNddo48aNmj59uiTp/fffb9a+RBrzIUtJ/L2GJEosAAANcOLECY0YMUL//d//He6utHjXXHONnn/+ee3bt6/ZPvP48eMaOXKkvv32W592t9utXr16WdtL///27jw66ur+//gz+zpZyAIhgQQIQkgIS0DWykGPVBaxspyCeLQI0qqnR+sRtNJjqa0FEbdT0FMWlaKFUy0SlE1ABQQFTEAKAmIIQhZCQvZ1ksn9/sGP+SUFLCOTmYS8HufMYSbzmc+87yufhPfc3PlMe5WWlma/3vQ0f9J+aQZZRESu+Pjmq93fFs+T21p17drV/ul8rvDYY49d0Rz/6le/AuDNN9/E07N9twOX32wKV565RNqn9v0TISIiZGRkYLFY7OtR/1tJSQn79u2joaHBxZXdvEJDQ13aIB85cgSACRMmsHTpUvLz83n77bcxxjB27FiX1dGaTZ48md///vfuLkNaCS2xEBFp55588kng0lkM/vnPfzJ9+vRm93/88ccAjBkzxuW13axCQ0OpqKjAZrPh5eXV4s/Xu3dvDh8+zMaNGzVDeg0ffPCBu0uQVkQzyCIi7dDQoUN56qmn+PDDD9m9e7f965c//OKyl156iQceeIDg4GCefvppV5d507JarQCsWLHCJc9XXl5OWlqammOR66QGWUSknTHGsH//fl5++WUmTZp0xf3vv/8+AO+88w7z5s0D4J577nHJTGd7kZ+fD8AjjzzC2LFjKS8vb9HnKysrIyQkpEWfQ+RmogZZRKSd2blzZ7PbTz75JL/85S/tt7dt28YPP/zAzJkz7V+73CiLczT9ZL2tW7faz6JQX1/PuHHjGD58OKWlpeTm5vLmm2/aG+prWbRoUbM3mgF89NFHPPHEE9x3333s3buXyMhI5w9E5CalNcgiIu1MRkZGs9u/+93viI6OZtmyZQwcOJBVq1axatUq+/2NjY3607yTdenShbCwMEpLSwH4/vvvsVqtHD9+nC1btgAQHh5u3/7YsWMsXbr0qvvasWOH/c1lCxYsYPXq1Zw5c+aK7Zo25SLy4zSDLCLSzlRXV+Ph4UF2djbLly8nLi4OX19fIiIisNlszbbds2ePmuMWcrk5viwzM5MLFy5cddtly5aRmJjIW2+9Zf8eVVVVsX37dl588UX7dn/605/szfHAgQPZtm0bL7zwAmfPnqV3794tMxCRm5BmkEVE2pHCwkKef/55OnXqREJCAg8//HCz+/97LezIkSNdWV67kpmZycCBA1m8eDHz5s1j2LBhzJo1q9k2U6ZMoaSkhJ07d5KVlcWsWbOYNWsWjz32GDt27ODkyZMATJw4kcmTJ3Py5EkCAgKYP3++/YWNzj4i4jg1yCIi7ciSJUsAsFgsV70/KSmJAwcOcPToURISElxYWfszYMAAjDHA/1/jvWrVKoYPH056ejplZWUEBAQQEBDAwIEDSUhIoKCggOPHj7Ns2TLg0uniXnzxRSZPnqw1xiJOpAZZRKQd2bBhA4B9net/W79+PR9//DHJycmuLKvdO3LkCKmpqQAkJycTGRlJZGQkeXl5hIeHk52dbd/26NGjZGRk8O233zJ16lQGDRrkrrJFblpqkEVE2qjq6mqeffZZ3nrrLQ4dOkSPHj2uua0xhtOnT/Pdd9/x17/+9ZrbxsbG8utf/7qlSpZr6Nu3L1arlXfffZfRo0f/6LYpKSmkpKS4qDKR9klv0hMRaaPmzp3L66+/TkVFBYmJiXh4eLBu3bpm2xhjSEtLw2KxkJiYCHDF6cCkdfDx8WHmzJla2iLSCqhBFhFpQ4wxPPfccxw+fJhDhw5dcf/06dPx8PDAw8OD8ePHc/LkSTIzM6mqqrJvc8cdd7iyZBGRNkcNsohIG3Lu3Dn+/Oc/M2DAAL788ksef/xx+vXrB3DF6dg2b97c7E/xf/zjHzHG6M1cIiL/g9Ygi4i0If/9IR8vvPAChYWFrFu3jnnz5nHhwgUWL15M//79efDBB+3nzM3NzaVz587uKFlEpM1Rgywi0oa888479utxcXEEBQURFBTEM888A0CnTp145ZVXgEsfYbx27VoiIiLUHIuIOEBLLERE2oDi4mKWLFnCxo0bef755zHGcO7cuR99zLJlywgJCWH+/PkuqlJE5OagGWQRkVYuPz+/2Qzw448/fl2PCw8Pp6SkBE9PzYWIiDhCvzVFRFqxTz75pNk5i1euXElISMh1P17NsYiI4zSDLCLiAsYYzp49y9KlSwkKCmLw4MGEhYUxYsQI+zY2m43vv/8eT09PPv30U/bu3cuaNWuIj49n9erVjBo1yo0jEBFpP9QgtzLHjh1j5MiRREZGsmvXLr2xRqQV++GHH5g5cyZvvfXWFR/ucO7cObZt20ZqairZ2dnMnTv3qmuGFyxYwO23387LL7/Mjh07mp2vGKBPnz589NFH+nAPEREXUoPcyixatIjS0lJKS0uJjY1l5MiRbNq0yf4n1TVr1rB8+XLGjh1LWloa/v7+VFdXk5qaSmRkJN26dXPzCERufitXrmThwoWcPn0agG7durFhwwa++uorioqKKCgo4KOPPmr2GB8fHyZPnszTTz/NggULqKqqYteuXSxYsIAFCxYAcN999zFs2DCMMQwePJjU1FQCAwNdPTwRkXZPDXIrcf78eR599FE+/PBDbr31Vg4cOADAF198QUpKCjNmzGDr1q0cPnwYuPTu9KsZO3YsCxcuJCkpCV9fX0pLSzl37hzx8fEEBwff0HrEixcvkpeXR1RUFB07drR/KEFtbS0lJSVERERgjMHPz+8nP4eIMYY33niDFStWMHToUO6++25sNhtRUVH2F4TGGCwWC9HR0cTExPyk56mvr6e8vBy49JebvXv3kpubS2hoKBaLhRMnTmCMwdPTk+zsbGw2G1VVVRQXF/PDDz8A8LOf/Yw9e/YA8Itf/AIAX19fQkJCmDJlCn379sXf35+kpCRuu+02QkNDAdi0aRNwaX3x22+/TXZ2NqtWrSI5OfmGshMREefwMMYYdxfhSoMGDeLrr792dxlXiImJ4fz58wDk5eXx8MMPs3//fqqqqqipqbFvN2nSJGbPnk10dDRZWVkEBQWRn59PSEgIK1euZPv27dd8Dh8fH+Lj4+nVqxfjx49n0qRJhIeHc/78eZYvX87QoUMZNWoUQUFBGGNoaGigqqqKDz74gH/961/s2rWLhoYGAAICAujQoQMhISGcOnXK/nUAb29vwsLC6N69O/X19dhsNiwWC0OGDCEhIYGoqCh8fHyIiYnB29sbX19f4uPjCQ8Pv66sLh+y//2pYVeTl5enZSrX0NjYSGVlJcHBwVitVnx9fTl//nyL5dXY2EhdXR21tbV4eHhgs9kwxuDj44PNZmPt2rV88cUXZGRkcOrUqeveb0BAAL169cLf3x9/f39qamoICgqiqKiInJwcampqCAgIwGaz0djYiM1mw2azUV9fT2Nj4zX3GxoaSmBgIJ6ensTExBAcHExwcDChoaH06dOHOXPmEBkZyf79+9m9ezdFRUXMmTOn2Rvq5Er6mXSM8nKM8nKM8rp2X9jmG+StW7fy+OOPY7PZmD17tv1k+dfS2hrkEydOMH/+fNavXw/A8ePH6d27N/X19TQ0NPD5558zbtw4Hn30UV555RUuXrx4zYPZarXy73//m1dffZWDBw8C0L17d7p3786IESOoq6vj6NGjbN++nbq6uqvuw9vbu1mze5mfnx+33XYbY8aMobq6mr1792KxWGhoaCA4OJg+ffrQ0NBgb4KKi4vJysrCZrMRGBjI999/z3fffXfNHLy8vIiJiSEhIYG4uDhCQ0OpqakhLy+P4OBgEhIS6N69O1arlfT0dDIzMxk/fjzR0dEUFxdTW1tL586diY2NpXPnzgQHBxMVFYUxhs6dOxMXF4e/vz8ApaWlhIaGXrXBbmxspLCwkOLiYurq6oiPjycsLOy6mnFH2Gw2ysvLOXHiBCEhIfj5+eHr63vFv15eXtf93MYYKisruXDhAgUFBezcuZMXXniBQYMGMW7cOAICAigvL+fChQucOXOGvXv3UlZWZn+8p6cnvr6+9ucLDw+nY8eOdOjQgdDQUCorKykrK6O8vJzi4mLKysrw8fEhNDSUgIAArFYrcOmFy+UGuLa21n65fP+PCQsLIy0tjQEDBjBv3jz+8Ic/0LFjRxITE4mMjKS2tpbg4GAAKioq+Prrr9m2bRudO3fGarVSW1trn2UODw+na9eu+Pn5YbVa8fLywsvLC09PT7y8vPDx8SEqKoqGhgbi4uIYPXq0fSlTTU3NdX/f9R+MY5SXY5SXY5SXY5TXTdog22w2brnlFrZv305cXByDBw9m7dq19OnT55qPcXWD/NVXX5GXl0dxcTElJSWUlJSQm5uLp6cnBQUFbNmyBYC77rqLv//973Tt2vVH9+fIwVxWVmb/k25TVquV999/n4yMDM6fP09mZia//e1v6datG3v27LE3SZ6engQGBtKzZ08mTJiAl5eX4wH8P42NjXz44Yd4eXnRq1cvioqKqKysxGazYbVaOXjwIJmZmZSVlfHtt99SUVFBREQEXbp0ob6+ntOnTzebSQcIDg7Gy8uLiIgI/Pz8yMnJoaKi4qrP7+PjQ48ePfD29ubo0aPExMRgsVioq6ujurqaxsZG+4xqfX39VR/v7++Pn58fgYGBhIWF2WdAL18aGxub3TbG4Ovri81mo6GhAavVSl1d3RV/FfgxHTt2JDg4GA8PD2pqamhoaMDHxwdfX1+MMdTX19sv1dXV1NbW/s99+vj4EB0dTY8ePQgKCmLIkCF4eHhQVFREfX09FouF+vp69u3bR3l5OSEhIVRVVREcHExISAihoaGEhYXZXyCVlpZitVrx9/e3jxsuNdyXZ3UDAgLs1/38/GhsbMTb2xtPT0/q6uqw2WyMGjWKIUOGXFcurYn+g3GM8nKM8nKM8nKM8rp2X9im1yAfOHCAxMRE+7u7p02bRnp6+o82yK72m9/8hm+++cZ+28vLi7CwMLy9vencuTPTp09nypQp3HvvvU6fpbxacwyX1kjOmDGDGTNmXHHfuHHjnFrDZZ6enkyePPma90+aNMl+/fIsaNN37RtjuHDhAv7+/oSEhFwzq6qqKk6fPo3NZqOwsJDvvvsOb29vDh48yOeff058fDwjRoygpqaGuro6/Pz8qKioIDIyEl9fXywWC7GxsURERODl5cWpU6eor6+nrq7OfikvL6eyshJPT0/7bGnT65dvw6UXI97e3valJD4+PgQFBWGxWLBYLHTp0gVjDHV1dVit1maXiooKsrKyKCoqokOHDgQGBuLt7U19fT1WqxUPDw97s+zj40NAQADR0dHNLqmpqRQUFFBbW2tfEvNjL3T0y1JERKSNN8i5ubl06dLFfjsuLo79+/dfsd3y5ctZvnw5cOnNcHl5eS6rcfHixXh4eBAWFkZoaChBQUFXbe7y8/Ova3+FhYXOLrFV8vf3v+r3qaqq6orTYP23iIgIAHuTGBUVxd133/2T6hg5cuRPelxrceHCBTw8PAgICKCmpuZ/zly3l+PLmZSZY5SXY5SXY5SXY5TXtbXpBvl6zZkzhzlz5gCXptJdOUPWEs+lGT7HKC/HKC/HKTPHKC/HKC/HKC/HKK+ra9OfQRobG9vsxPs5OTnExsa6sSIRERERaevadIM8ePBgTp06RXZ2NlarlXXr1jFx4kR3lyUiIiIibVibXmLh7e3N0qVL+fnPf47NZuOhhx7SifZFRERE5Ia06QYZLp11oaXOvCAiIiIi7U+bXmIhIiIiIuJsapBFRERERJpQgywiIiIi0oQaZBERERGRJtQgi4iIiIg0oQZZRERERKQJNcgiIiIiIk2oQRYRERERaUINsoiIiIhIE2qQRURERESaUIMsIiIiItKEhzHGuLsIV4qMjCQhIcHdZfxkhYWFREVFubuMNkN5OUZ5OU6ZOUZ5OUZ5OUZ5OUZ5wZkzZygqKrri6+2uQW7rBg0axNdff+3uMtoM5eUY5eU4ZeYY5eUY5eUY5eUY5XVtWmIhIiIiItKEGmQRERERkSa8FixYsMDdRYhj0tLS3F1Cm6K8HKO8HKfMHKO8HKO8HKO8HKO8rk5rkEVEREREmtASCxERERGRJtQgi4iIiIg0oQbZzc6dO8fo0aPp06cPycnJvP766wAUFxdz55130rNnT+68805KSkoAOHHiBMOGDcPPz48lS5ZcsT+bzcaAAQOYMGGCS8fhKs7MKyEhgb59+9K/f38GDRrk8rG4gjPzKi0tZcqUKfTu3ZukpCS+/PJLl4/HFZyV2cmTJ+nfv7/9EhISwmuvveaWMbUkZx5jr776KsnJyaSkpDB9+nRqa2tdPp6W5sy8Xn/9dVJSUkhOTr4pjy1wPK/33nuP1NRU+vbty/Dhw/nmm2/s+9q6dSu9evUiMTGRRYsWuWU8Lc2ZeT300ENER0eTkpLilrG4nRG3ysvLMxkZGcYYY8rLy03Pnj3NsWPHzNy5c83ChQuNMcYsXLjQzJs3zxhjTEFBgTlw4IB59tlnzUsvvXTF/l5++WUzffp0M378eNcNwoWcmVd8fLwpLCx07QBczJl5PfDAA2bFihXGGGPq6upMSUmJC0fiOs7+mTTGmIaGBtOxY0dz5swZ1wzChZyVV05OjklISDDV1dXGGGOmTp1q3n77bdcOxgWcldd//vMfk5ycbKqqqkx9fb254447zKlTp1w/oBbmaF579+41xcXFxhhjNm/ebG699VZjzKWfwe7du5usrCxTV1dnUlNTzbFjx9wwopblrLyMMWbXrl0mIyPDJCcnu3gUrYNmkN0sJiaGgQMHAmCxWEhKSiI3N5f09HQefPBBAB588EE2bNgAQHR0NIMHD8bHx+eKfeXk5LBp0yZmz57tugG4mDPzag+clVdZWRm7d+9m1qxZAPj6+hIWFubCkbhOSxxjO3fupEePHsTHx7f8AFzMmXk1NDRQU1NDQ0MD1dXVdO7c2XUDcRFn5XX8+HGGDBlCYGAg3t7ejBo1ivXr17t2MC7gaF7Dhw8nPDwcgKFDh5KTkwPAgQMHSExMpHv37vj6+jJt2jTS09PdMKKW5ay8AG677TY6dOjg4hG0HmqQW5EzZ85w6NAhhgwZQkFBATExMQB06tSJgoKC//n4J554gsWLF+Pp2T6+rTeal4eHB2PGjCEtLY3ly5e3dLludyN5ZWdnExUVxcyZMxkwYACzZ8+mqqrKFWW71Y0eY5etW7eO6dOnt1SZrcaN5BUbG8tTTz1F165diYmJITQ0lDFjxriibLe5kbxSUlLYs2cPFy9epLq6ms2bN3Pu3DlXlO02jua1atUqxo4dC0Bubi5dunSx3xcXF0dubq5rCneTG8lL1CC3GpWVlUyePJnXXnuNkJCQZvd5eHjg4eHxo4//+OOPiY6ObjfnM7zRvAC++OILMjMz2bJlC8uWLWP37t0tVa7b3WheDQ0NZGZm8sgjj3Do0CGCgoJu2jV8lznjGAOwWq1s3LiRqVOntkSZrcaN5lVSUkJ6ejrZ2dnk5eVRVVXFu+++25Ilu9WN5pWUlMTTTz/NmDFjuOuuu+jfvz9eXl4tWbJbOZrXZ599xqpVq3jxxRddWWarobxunBrkVqC+vp7JkyczY8YMJk2aBEDHjh3Jz88HID8/n+jo6B/dx969e9m4cSMJCQlMmzaNTz/9lPvvv7/Fa3cHZ+QFl2as4NKfMO+9914OHDjQckW7kTPyiouLIy4ujiFDhgAwZcoUMjMzW7ZwN3LWMQawZcsWBg4cSMeOHVusXndzRl47duygW7duREVF4ePjw6RJk9i3b1+L1+4Ozjq+Zs2aRUZGBrt37yY8PJxbbrmlRet2F0fzOnLkCLNnzyY9PZ2IiAjg0u/7pjPsOTk59v8DbjbOyEvUILudMYZZs2aRlJTEk08+af/6xIkTWb16NQCrV6/mnnvu+dH9LFy4kJycHM6cOcO6deu4/fbbb8rZF2flVVVVRUVFhf36J598clO+U9dZeXXq1IkuXbpw8uRJ4NKa2j59+rRc4W7krMwuW7t27U29vMJZeXXt2pWvvvqK6upqjDHs3LmTpKSkFq3dHZx5fF24cAGAs2fPsn79eu67776WKdqNHM3r7NmzTJo0iTVr1jR7wTB48GBOnTpFdnY2VquVdevWMXHiRNcOxgWclZegs1i42549ewxg+vbta/r162f69etnNm3aZIqKisztt99uEhMTzR133GEuXrxojDEmPz/fxMbGGovFYkJDQ01sbKwpKytrts/PPvvspj2LhbPyysrKMqmpqSY1NdX06dPH/OUvf3HzyFqGM4+vQ4cOmbS0NNO3b19zzz332N/5fLNxZmaVlZWmQ4cOprS01J1DalHOzOu5554zvXr1MsnJyeb+++83tbW17hxai3BmXiNHjjRJSUkmNTXV7Nixw53DajGO5jVr1iwTFhZm3zYtLc2+r02bNpmePXua7t2763f+deQ1bdo006lTJ+Pt7W1iY2PNypUr3TUst9BHTYuIiIiINKElFiIiIiIiTahBFhERERFpQg2yiIiIiEgTapBFRERERJpQgywiIiIi0oQaZBGRm0RpaSlvvPEGAHl5eUyZMsXNFYmItE06zZuIyE3izJkzTJgwgaNHj7q7FBGRNs3b3QWIiIhzPPPMM2RlZdG/f3969uzJ8ePHOXr0KO+88w4bNmygqqqKU6dO8dRTT2G1WlmzZg1+fn5s3ryZDh06kJWVxWOPPUZhYSGBgYGsWLGC3r17u3tYIiIupyUWIiI3iUWLFtGjRw8OHz7MSy+91Oy+o0ePsn79eg4ePMj8+fMJDAzk0KFDDBs2jH/84x8AzJkzh7/97W9kZGSwZMkSHn30UXcMQ0TE7TSDLCLSDowePRqLxYLFYiE0NJS7774bgL59+3LkyBEqKyvZt28fU6dOtT+mrq7OXeWKiLiVGmQRkXbAz8/Pft3T09N+29PTk4aGBhobGwkLC+Pw4cPuKlFEpNXQEgsRkZuExWKhoqLiJz02JCSEbt268f777wNgjOGbb75xZnkiIm2GGmQRkZtEREQEI0aMICUlhblz5zr8+Pfee49Vq1bRr18/kpOTSU9Pb4EqRURaP53mTURERESkCc0gi4iIiIg0oQZZRERERKQJNcgiIiIiIk2oQRYRERERaUINsoiIiIhIE2qQRURERESaUIMsIiIiItLE/wFK6Z8vYtsO7AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijknhx20evcK"
      },
      "source": [
        ""
      ],
      "execution_count": 195,
      "outputs": []
    }
  ]
}